{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhd-sJ-CqlpE"
      },
      "source": [
        "#0. Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nxuPRrvZrMrM"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Importamos las librerías habituales\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      4\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpylab\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "# Importamos las librerías habituales\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting google.colab\n",
            "  Downloading google-colab-1.0.0.tar.gz (72 kB)\n",
            "     ---------------------------------------- 0.0/72.9 kB ? eta -:--:--\n",
            "     ---------- --------------------------- 20.5/72.9 kB 330.3 kB/s eta 0:00:01\n",
            "     --------------------- ---------------- 41.0/72.9 kB 393.8 kB/s eta 0:00:01\n",
            "     -------------------------------------- 72.9/72.9 kB 574.7 kB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting google-auth~=1.4.0 (from google.colab)\n",
            "  Downloading google_auth-1.4.2-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting ipykernel~=4.6.0 (from google.colab)\n",
            "  Downloading ipykernel-4.6.1-py3-none-any.whl.metadata (981 bytes)\n",
            "Collecting ipython~=5.5.0 (from google.colab)\n",
            "  Downloading ipython-5.5.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting notebook~=5.2.0 (from google.colab)\n",
            "  Downloading notebook-5.2.2-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting six~=1.12.0 (from google.colab)\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting pandas~=0.24.0 (from google.colab)\n",
            "  Downloading pandas-0.24.2.tar.gz (11.8 MB)\n",
            "     ---------------------------------------- 0.0/11.8 MB ? eta -:--:--\n",
            "     - -------------------------------------- 0.4/11.8 MB 8.0 MB/s eta 0:00:02\n",
            "     --- ------------------------------------ 1.0/11.8 MB 10.6 MB/s eta 0:00:02\n",
            "     ----- ---------------------------------- 1.5/11.8 MB 12.2 MB/s eta 0:00:01\n",
            "     ------- -------------------------------- 2.1/11.8 MB 12.5 MB/s eta 0:00:01\n",
            "     --------- ------------------------------ 2.7/11.8 MB 13.2 MB/s eta 0:00:01\n",
            "     ----------- ---------------------------- 3.3/11.8 MB 13.0 MB/s eta 0:00:01\n",
            "     ------------- -------------------------- 3.9/11.8 MB 13.8 MB/s eta 0:00:01\n",
            "     --------------- ------------------------ 4.6/11.8 MB 13.9 MB/s eta 0:00:01\n",
            "     ----------------- ---------------------- 5.2/11.8 MB 13.9 MB/s eta 0:00:01\n",
            "     ------------------- -------------------- 5.8/11.8 MB 13.7 MB/s eta 0:00:01\n",
            "     --------------------- ------------------ 6.3/11.8 MB 14.0 MB/s eta 0:00:01\n",
            "     ----------------------- ---------------- 6.9/11.8 MB 13.8 MB/s eta 0:00:01\n",
            "     ------------------------- -------------- 7.5/11.8 MB 13.8 MB/s eta 0:00:01\n",
            "     --------------------------- ------------ 8.1/11.8 MB 13.9 MB/s eta 0:00:01\n",
            "     ----------------------------- ---------- 8.7/11.8 MB 13.9 MB/s eta 0:00:01\n",
            "     ------------------------------- -------- 9.3/11.8 MB 14.2 MB/s eta 0:00:01\n",
            "     --------------------------------- ----- 10.0/11.8 MB 14.0 MB/s eta 0:00:01\n",
            "     ---------------------------------- ---- 10.6/11.8 MB 14.5 MB/s eta 0:00:01\n",
            "     ------------------------------------ -- 11.2/11.8 MB 14.2 MB/s eta 0:00:01\n",
            "     --------------------------------------  11.8/11.8 MB 14.2 MB/s eta 0:00:01\n",
            "     --------------------------------------- 11.8/11.8 MB 13.9 MB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'error'\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Getting requirements to build wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [30 lines of output]\n",
            "      <string>:12: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "      Traceback (most recent call last):\n",
            "        File \"c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
            "          main()\n",
            "        File \"c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
            "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
            "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\n",
            "          return hook(config_settings)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\pip-build-env-q3xs4nvi\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 325, in get_requires_for_build_wheel\n",
            "          return self._get_build_requires(config_settings, requirements=['wheel'])\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\pip-build-env-q3xs4nvi\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 295, in _get_build_requires\n",
            "          self.run_setup()\n",
            "        File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\pip-build-env-q3xs4nvi\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 487, in run_setup\n",
            "          super().run_setup(setup_script=setup_script)\n",
            "        File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\pip-build-env-q3xs4nvi\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 311, in run_setup\n",
            "          exec(code, locals())\n",
            "        File \"<string>\", line 732, in <module>\n",
            "        File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\pip-install-p6ehfjkh\\pandas_46aa50333fcf4f19945fed4253f50122\\versioneer.py\", line 1409, in get_version\n",
            "          return get_versions()[\"version\"]\n",
            "                 ^^^^^^^^^^^^^^\n",
            "        File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\pip-install-p6ehfjkh\\pandas_46aa50333fcf4f19945fed4253f50122\\versioneer.py\", line 1343, in get_versions\n",
            "          cfg = get_config_from_root(root)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\pip-install-p6ehfjkh\\pandas_46aa50333fcf4f19945fed4253f50122\\versioneer.py\", line 399, in get_config_from_root\n",
            "          parser = configparser.SafeConfigParser()\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "      AttributeError: module 'configparser' has no attribute 'SafeConfigParser'. Did you mean: 'RawConfigParser'?\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: subprocess-exited-with-error\n",
            "\n",
            "× Getting requirements to build wheel did not run successfully.\n",
            "│ exit code: 1\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O32QYyzuxsa3"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##Cómo levantar el df limpio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZYd3WMrxzWQ"
      },
      "outputs": [],
      "source": [
        "directory = '/content/drive/My Drive/DatosTP/limp'\n",
        "df = pd.read_pickle('/content/drive/My Drive/DatosTP/limp1.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zuu3ktBPx3g7"
      },
      "outputs": [],
      "source": [
        "for i in range(2,14):\n",
        "  path = directory + str(i) + '.pickle'\n",
        "  df1 = pd.read_pickle(path)\n",
        "  df = df.append(df1)\n",
        "\n",
        "df\n",
        "dfRank = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OkBeMAatvrb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHOkoltntvxZ"
      },
      "outputs": [],
      "source": [
        "df[\"raices\"] = df[\"raicesl\"]\n",
        "df[\"text\"] = df[\"dream\"]\n",
        "\n",
        "#Junta y arma un solo \"texto\" con las raíces\n",
        "df['raices_unidas'] = df['raices'].apply(\" \".join)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YH8ySeQx8x6"
      },
      "source": [
        "Es casi igual que antes, solo que con 13 archivos limp#.pickle  \n",
        "También cambiaron las columnas:  \n",
        "['cohesion', 'intent', 'lucidity', 'raicesl', 'rating', 'technique',\n",
        "       'url', 'user', 'dream', 'additional_comments', 'themes', 'settings',\n",
        "       'characters', 'emotions', 'activities']\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkGdGHdsyQwu"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "##Cómo levantar el df Viejo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPrqamGVqnMp"
      },
      "outputs": [],
      "source": [
        "#Correr solo esto para tener un df de 9k entradas\n",
        "#directory = '/content/drive/My Drive/Dreams/data'\n",
        "#df = pd.read_pickle('/content/drive/My Drive/Dreams/data1.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIXc3X67rGYd"
      },
      "outputs": [],
      "source": [
        "#Todos los pickles (ESTÁ PUESTO HASTA EL 5: TODOS ES HASTA EL 15)\n",
        "#for i in range(2,5):\n",
        "#  path = directory + str(i) + '.pickle'\n",
        "#  df1 = pd.read_pickle(path)\n",
        "#  df = df.append(df1, ignore_index=True)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuxmk0zpt5Lu"
      },
      "source": [
        "#1. Limpieza previa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfCbjX9z8G2T"
      },
      "outputs": [],
      "source": [
        "#Hay algunos repetidos...\n",
        "print(df[\"raices\"][35427])\n",
        "print(df[\"raices\"][35428])\n",
        "print(df[\"url\"][35427])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnwNfoS0t_H3"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVZ16bftQmwW"
      },
      "source": [
        "# 2. ¿Qué sueña la gente?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtcKmEbMRNMG"
      },
      "source": [
        "#Análisis de sentimientos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amdUXFVfQ5Mc"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "#no sé si usamos todo esto\n",
        "nltk.download([\n",
        "     \"names\",\n",
        "     \"stopwords\",\n",
        "     \"state_union\",\n",
        "     \"twitter_samples\",\n",
        "     \"movie_reviews\",\n",
        "     \"averaged_perceptron_tagger\",\n",
        "     \"vader_lexicon\",\n",
        "     \"punkt\",\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zS_RyKmcSMsj"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "df[\"sentiment\"] = df[\"text\"].apply(sia.polarity_scores)   #TARDA BASTANTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI2uH3JDhuAG"
      },
      "outputs": [],
      "source": [
        "df[\"sentiment\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ww1RCcHzhxDY"
      },
      "outputs": [],
      "source": [
        "df[\"neg\"] = pd.Series\n",
        "df[\"pos\"] = pd.Series\n",
        "\n",
        "for i in df[\"sentiment\"].index:\n",
        "  df[\"neg\"][i] = df[\"sentiment\"][i][\"neg\"]\n",
        "  df[\"pos\"][i] = df[\"sentiment\"][i][\"pos\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VuiYa0DtonPY"
      },
      "outputs": [],
      "source": [
        "#Los que tienen más de 0,5 en positivo\n",
        "df[df[\"pos\"]>0.5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LbjaaIgFsL9p"
      },
      "outputs": [],
      "source": [
        "#Más de 0,7 en negativo\n",
        "df[df[\"neg\"]>0.7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "03PJKvy5r9mz"
      },
      "outputs": [],
      "source": [
        "#Histograma negativo\n",
        "df[\"neg\"].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4C05KbvEOOP"
      },
      "source": [
        "# Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "m9U19bL7ELBB"
      },
      "outputs": [],
      "source": [
        "from gensim.models.phrases import Phrases, Phraser\n",
        "\n",
        "input = [row.split() for row in df['raices_unidas']] # separamos en una lista\n",
        "\n",
        "phrases = Phrases(input, min_count=30, progress_per=1000)\n",
        "\n",
        "bigram = Phraser(phrases)\n",
        "\n",
        "sentences = bigram[input] # obtenemos las palabras junto con bigramas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TfxgpxyFEWfn",
        "outputId": "fa75124a-f51e-458b-8d22-35213df17ad4"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-bac3b0c981a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m### ENTRENA EL MODELO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m### PRECOMPUTA DISTANCIAS (mas rapido)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import multiprocessing\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "cores = multiprocessing.cpu_count()\n",
        "\n",
        "w2v_model = Word2Vec(min_count=20, # ignora palabras cuya frecuencia es menor a esta\n",
        "                     window=2, # tamanio de la ventana de contexto\n",
        "                     size=300, # dimension del embedding\n",
        "                     sample=6e-5, # umbral para downsamplear palabras muy frecuentes\n",
        "                     alpha=0.03, # tasa de aprendizaje inicial (entrenamiento de la red neuronal)\n",
        "                     min_alpha=0.0007, # tasa de aprendizaje minima\n",
        "                     negative=20, # penalidad de palabras muy frecuentes o poco informaitvas\n",
        "                     workers=cores) # numero de cores para entrenar el modelo\n",
        "\n",
        "w2v_model.build_vocab(sentences, progress_per=10000) # construye el vocabulario\n",
        "\n",
        "### ENTRENA EL MODELO\n",
        "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
        "### PRECOMPUTA DISTANCIAS (mas rapido)\n",
        "w2v_model.init_sims(replace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5oubl29FIW3"
      },
      "outputs": [],
      "source": [
        "w2v_model.wv.most_similar(positive=[\"sex\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHSE17nt1tv8"
      },
      "source": [
        "# Tópicos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2ozjzgX1r8X"
      },
      "outputs": [],
      "source": [
        "# Importamos las librerías habituales\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY8U24471xvK"
      },
      "outputs": [],
      "source": [
        "# Objetos de sklearn para hacer tópicos\n",
        "from sklearn.feature_extraction.text import CountVectorizer # Contador de frecuencia\n",
        "from sklearn.feature_extraction.text import TfidfTransformer # Creador de tf-idf\n",
        "\n",
        "# Algoritmos de descomposición de tópicos\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8lauNvq10bC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Creamos el objeto contador de palabras, pidiéndole que remueve\n",
        "# las stopwords, los términos que aparecen en un único documento (min_df)\n",
        "# y los términos que aparecen en más del 70% de los documentos (max_df).\n",
        "# Esto es para eliminar palabras raras (o errores de tipeo) y\n",
        "# términos que seguramente son stopwords no incluídos en la lista\n",
        "count = CountVectorizer(min_df = 2, max_df = 0.70, stop_words = stopwords)\n",
        "\n",
        "# Ajustamos con los datos. Acá especificamente creamos una matriz documentos-términos\n",
        "x_count = count.fit_transform(df['raices_unidas'])\n",
        "\n",
        "# Dimensions de la matriz doc-tér\n",
        "print(x_count.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMx8OmJE12AG"
      },
      "outputs": [],
      "source": [
        "# Creamos el objeto tf-idf. Le decimos además que devuelva los\n",
        "# vectores documento con norma euclídea igual a 1 (norm = 'l2')\n",
        "tfidf = TfidfTransformer(norm = 'l2')\n",
        "\n",
        "# Creamos la matriz tf-idf a partir de la matriz de frecuencias\n",
        "x_tfidf = tfidf.fit_transform(x_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wtm6HqrD13lI"
      },
      "outputs": [],
      "source": [
        "# Elijamos la cantidad de tópicos\n",
        "n_components = 5\n",
        "\n",
        "# Construímos el objeto NMF con los tópicos indicados\n",
        "nmf = NMF(n_components = n_components)\n",
        "\n",
        "# Aplicamos sobre nuestros datos\n",
        "x_nmf = nmf.fit_transform(x_tfidf)\n",
        "\n",
        "# Dimensión de la matriz transformada\n",
        "print(x_nmf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiosTeSe15Js"
      },
      "outputs": [],
      "source": [
        "# Objeto índice: término de nuestro vocabulario\n",
        "vocabulary = {item: key for key, item in count.vocabulary_.items()}\n",
        "\n",
        "# Para cada componente\n",
        "for n in range(n_components):\n",
        "\n",
        "  # Ordenamos una lista del largo de nuestro vocabulario según el peso en cada componente y nos quedamos con los primeros 10\n",
        "  list_sorted = sorted(range(nmf.components_.shape[1]), reverse = True, key = lambda x: nmf.components_[n][x])[:10]\n",
        "\n",
        "  # Printeamos los términos asociados a los valores más grande de cada una de las componentes\n",
        "  print(', '.join([vocabulary[i] for i in list_sorted]))\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mAf0IT52_2z"
      },
      "outputs": [],
      "source": [
        "# WordClouds\n",
        "wc_atributos = {'height' : 800,\n",
        "                'width' : 1200,\n",
        "                'background_color' : 'white',\n",
        "                'max_words' : 20\n",
        "                } # Defino los parámetros que les voy a pasar a los wordclouds\n",
        "\n",
        "# Creo la figura\n",
        "fig, axs = plt.subplots(n_components, figsize = (6,20))\n",
        "\n",
        "# Recorro para todas las componentes\n",
        "for n in range(n_components):\n",
        "\n",
        "  # 10 términos más pesados\n",
        "  list_sorted = sorted(range(len(vocabulary)), reverse = True, key = lambda x: nmf.components_[n][x])[:10]\n",
        "\n",
        "  # Diccionario término: peso\n",
        "  comp_dict = {vocabulary[i]: nmf.components_[n][i] for i in list_sorted}\n",
        "\n",
        "  # Creo el wordlcoud\n",
        "  wc = WordCloud(**wc_atributos # De esta forma, le estoy diciendo a la función que expanda el diccionario de atributos de forma tal de que entienda lo que quiero que haga\n",
        "                 ).generate_from_frequencies(comp_dict)\n",
        "\n",
        "  axs[n].set_title('Tópico {}'.format(n))\n",
        "  axs[n].imshow(wc)\n",
        "  axs[n].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP_cQ86e66_L"
      },
      "outputs": [],
      "source": [
        "# Normalizador\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# Creamos un objeto para normalizar a que la suma dé 1\n",
        "norm = Normalizer('l1')\n",
        "\n",
        "# Sobreescribimos sobre la matriz de documentos-tópicos\n",
        "x_nmf = norm.fit_transform(x_nmf)\n",
        "\n",
        "# Guardemos en el dataframe esta información\n",
        "for n in range(n_components):\n",
        "  df['nmf_comp{}'.format(n)] = x_nmf[:,n]\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPbKrfPS69D6"
      },
      "outputs": [],
      "source": [
        "df_metodo = df.groupby('technique').mean()\n",
        "\n",
        "# Inspeccionemoslo\n",
        "df_metodo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQhYfa_I6_e0"
      },
      "outputs": [],
      "source": [
        "# El eje x es la década\n",
        "x = df_metodo.index\n",
        "\n",
        "# El eje y son las distribuciones\n",
        "y = df_metodo[['nmf_comp{}'.format(i) for i in range(n_components)]].to_numpy()\n",
        "\n",
        "plt.figure(figsize = (8,5))\n",
        "plt.stackplot(x, y.T) # Stackplot: sirve para graficar distribuciones\n",
        "#plt.xlim([0, 90])\n",
        "plt.ylim([0, 1.00])\n",
        "plt.yticks([])\n",
        "plt.xlabel('Método')\n",
        "plt.legend(['Tópico {}'.format(i) for i in range(n_components)], loc = (1.05, 0.60))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiFXIVocRQ_3"
      },
      "source": [
        "# 3. ¿Cómo sueña?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kV1WjPQF_3I"
      },
      "outputs": [],
      "source": [
        "#Nos quedamos solo con los que contestaron green o red\n",
        "index_red = df[\"intent\"]==\"red\"\n",
        "index_green = df[\"intent\"]==\"green\"\n",
        "\n",
        "index_or = index_green + index_red\n",
        "df = df[index_or]\n",
        "\n",
        "#Tabla de contingencia de lucidity e intent\n",
        "pd.crosstab(df[\"intent\"], df[\"lucidity\"], margins=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czbpNg1h6cwK"
      },
      "outputs": [],
      "source": [
        "#Qué técnicas usaron\n",
        "df[\"technique\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1zhuRXuEqy0"
      },
      "outputs": [],
      "source": [
        "#Tabla de contingencia de técnica e intent\n",
        "pd.crosstab(df[\"intent\"], df[\"technique\"], margins = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7s2I_OnEsWx"
      },
      "outputs": [],
      "source": [
        "#Tabla de contingencia de lucidity y técnica\n",
        "pd.crosstab(df[\"lucidity\"], df[\"technique\"], margins = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sJ37GPrtLGl"
      },
      "source": [
        "#Nubes de palabras por intention: green/red"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCzldZNY8sQB"
      },
      "outputs": [],
      "source": [
        "#Me quedo con las raíces\n",
        "raices = df[\"raices_unidas\"].transpose()\n",
        "raices_green = df[\"raices_unidas\"][df[\"intent\"]==\"green\"].transpose()\n",
        "raices_red = df[\"raices_unidas\"][df[\"intent\"]==\"red\"].transpose()\n",
        "\n",
        "#Las junto todos en un solo texto de raices\n",
        "todos_textos_green = \" \".join(raices_green)\n",
        "todos_textos_red = \" \".join(raices_red)\n",
        "\n",
        "#Las cuento\n",
        "print(\"Hay en total:\", raices.size, \"relatos\")  #hay vacíos?\n",
        "print(\"De los cuales son red:\", raices_red.size)\n",
        "print(\"La cant de green son:\", raices_green.size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jSMhiPwb0Uj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Cargamos del paquete nltk las stopwords a la lista \"stopwords\"\n",
        "import nltk\n",
        "nltk.download('stopwords') # hay que descargar este modulo en particular\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "print(stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez-PO0hZjEE8"
      },
      "outputs": [],
      "source": [
        "#Limpieza adicional para este set\n",
        "stopwords = stopwords + [\"quot\", \"nbsp\", \"additional\", \"comment\", \"rsquo\", \"rdquo\", \"unfamiliar character\"] #add emotions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI4v_Jh3BoE2"
      },
      "outputs": [],
      "source": [
        "print(df[\"raices\"][151])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfXz1KRD8v6N"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud # importo la funcion WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Creo el objeto WordCloud sacando la lista de stopwords\n",
        "wc = WordCloud(stopwords=stopwords, background_color=\"white\", colormap=\"Dark2\",\n",
        "               max_font_size=150, random_state=42)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [16,12] # tamaño de los plots\n",
        "\n",
        "#Genero green\n",
        "wc.generate(todos_textos_green)\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Green sin stopwords\")\n",
        "plt.show()\n",
        "\n",
        "#Genero red\n",
        "wc.generate(todos_textos_red)\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Red sin stopwords\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KTtwrenzMhQ"
      },
      "outputs": [],
      "source": [
        "# Ahora con stopwords\n",
        "wc = WordCloud(background_color=\"white\", colormap=\"Dark2\",\n",
        "               max_font_size=150, random_state=42)\n",
        "\n",
        "wc.generate(todos_textos_green)  # acá le pido que genere los WC a partir del texto de cada año\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Green con stopwords\")\n",
        "plt.show()\n",
        "\n",
        "wc.generate(todos_textos_red)  # acá le pido que genere los WC a partir del texto de cada año\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Red con stopwords\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJDPE6GMoeiA"
      },
      "outputs": [],
      "source": [
        "#Armo listas de raices green/red con y sin stopwords:\n",
        "#Primero con stopwords:\n",
        "lista_raices_green = df[\"raices\"][df[\"intent\"]==\"green\"].transpose()\n",
        "lista_raices_red = df[\"raices\"][df[\"intent\"]==\"red\"].transpose()\n",
        "\n",
        "#Función que remueve de una lista de strings los elementos en stopwords\n",
        "def sw_remover3(lista):\n",
        "   return [value for value in lista if not value in stopwords]\n",
        "\n",
        "#Aplico la función y lo guardo en una nueva columna\n",
        "lista_raices_green_sinsw = lista_raices_green.apply(sw_remover3)\n",
        "lista_raices_red_sinsw = lista_raices_red.apply(sw_remover3)\n",
        "\n",
        "# Se borraron los stopwords, por ejemplo:\n",
        "print(lista_raices_red[4])\n",
        "print(lista_raices_red_sinsw[4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL8YOGcCxJZm"
      },
      "source": [
        "Sobre el dataset:  \n",
        "-1 significa que no había datos al scrapear  \n",
        "No hay Nan entre los elementos.  \n",
        "Hay ruido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTzj_zfKzZw-"
      },
      "outputs": [],
      "source": [
        "# Importamos las librerías habituales\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsntAzeVzhd4"
      },
      "outputs": [],
      "source": [
        "# Objetos de sklearn para hacer tópicos\n",
        "from sklearn.feature_extraction.text import CountVectorizer # Contador de frecuencia\n",
        "from sklearn.feature_extraction.text import TfidfTransformer # Creador de tf-idf\n",
        "\n",
        "# Algoritmos de descomposición de tópicos\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4xPEhPffkv8"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftEs0t_NfonF"
      },
      "source": [
        "#RANKEO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRoRsAs-fr4_"
      },
      "outputs": [],
      "source": [
        "\n",
        "ratings = dfRank[\"rating\"].value_counts().to_dict()\n",
        "\n",
        "print(\"Total de sueños no rankeados = \" + str(ratings[-1]))\n",
        "print(\"Total de sueños rankeados = \" + str(ratings[5]+ratings[4]+ratings[3]+ratings[2]+ratings[1]))\n",
        "print(\"Total de sueños rankeados positivos = \" + str(ratings[5]+ratings[4]))\n",
        "print(\"Total de sueños rankeados negativos = \" + str(ratings[2]+ratings[1]))\n",
        "print(\"Total de sueños rankeados neutros = \"+str(ratings[3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20ogg03eZQk6"
      },
      "source": [
        "Creo los dataframes segun rankeo positivo o negativo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GC6XZQSiWQ_Y"
      },
      "outputs": [],
      "source": [
        "dfRank = dfRank.loc[dfRank[\"rating\"]>0] #elimino los suenios no rankeados == -1\n",
        "dfPosRank = dfRank.loc[dfRank[\"rating\"]>3] #asumo rank pos == mayor a 3\n",
        "dfNegRank = dfRank.loc[dfRank[\"rating\"]<3] #asumo rank neg == menor a 3\n",
        "\n",
        "rank = dfRank[\"user\"].value_counts().to_dict()\n",
        "rankPos = dfPosRank[\"user\"].value_counts().to_dict()\n",
        "rankNeg = dfNegRank[\"user\"].value_counts().to_dict()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzHOfQ9sQIY-"
      },
      "source": [
        "##Son los suenios rankeados altos de los mismos usuarios?\n",
        "SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZ9v_XtAQGeN"
      },
      "outputs": [],
      "source": [
        "data = list(rankPos.values())\n",
        "\n",
        "plt.pie(data, labels= rankPos.keys())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgHoc0zyWihk"
      },
      "source": [
        "## quiero ver cuantos usuarios escribieron que porcentaje del total\n",
        " es decir, se que aproximadamente 30 usuarios escribieron el 50% del total de los rankeados positivos, 200 usuarios el 5% y 8000 usuarios el otro 45%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4cdPaVUWu4T"
      },
      "outputs": [],
      "source": [
        "cantUsuarios = np.zeros(5)\n",
        "cantPublicaciones = np.zeros(5)\n",
        "etiquetas = [\"1-10\",\"11-30\",\"31-50\",\"51-100\",\"101-453\"]\n",
        "\n",
        "\n",
        "keys = list(rankPos.keys())\n",
        "analizar5porciento = []\n",
        "analizar95porciento = []\n",
        "\n",
        "\n",
        "for i in range(len(rankPos.keys())):\n",
        "  key = keys[i]\n",
        "  if(rankPos[key] < 11):\n",
        "    cantPublicaciones[0] += rankPos[key]\n",
        "    cantUsuarios[0] += 1\n",
        "    #para analisis del otro parte\n",
        "    analizar95porciento.append(key)\n",
        "  elif(rankPos[key] < 31):\n",
        "    cantPublicaciones[1] += rankPos[key]\n",
        "    cantUsuarios[1] += 1\n",
        "    #para mi grafico d analisis ams adelante\n",
        "    analizar5porciento.append(key)\n",
        "  elif(rankPos[key] < 51):\n",
        "    cantPublicaciones[2] += rankPos[key]\n",
        "    cantUsuarios[2] += 1\n",
        "    #para mi grafico d analisis ams adelante\n",
        "    analizar5porciento.append(key)\n",
        "  elif(rankPos[key] < 101):\n",
        "    cantPublicaciones[3] += rankPos[key]\n",
        "    cantUsuarios[3] += 1\n",
        "    #para mi grafico d analisis ams adelante\n",
        "    analizar5porciento.append(key)\n",
        "  else:\n",
        "    cantPublicaciones[4] += rankPos[key]\n",
        "    cantUsuarios[4] += 1\n",
        "    #para mi grafico d analisis ams adelante\n",
        "    analizar5porciento.append(key)\n",
        "\n",
        "\n",
        "for i in range(len(cantUsuarios)):\n",
        "  cantUsuarios[i] = str(cantUsuarios[i])\n",
        "\n",
        "\n",
        "print(cantPublicaciones,cantUsuarios)\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(nrows = 1, ncols = 2)\n",
        "axs[0].set_title(\"Suenios\")\n",
        "axs[0].pie(cantPublicaciones, labels = cantUsuarios ,colors = plt.get_cmap('Set3').colors, autopct='%1.2f%%')\n",
        "#axs[1].set_title[\"Usuarios\"]\n",
        "axs[1].pie(cantUsuarios, labels = etiquetas ,colors = plt.get_cmap('Set3').colors, autopct='%1.2f%%')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKns7kaXpGbp"
      },
      "source": [
        "grafico 1: porcentaje de entradas y usuarios que las escribieron/// suma de publicaciones que hicieron los usuarios entre rango\n",
        "\n",
        "grafico 2: usuarios sobre entradas que publicaron\n",
        "esto me lleva a querer analizar a este 5% de los usuarios///cantidad de  usuarios con x entradas entre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX-vLt9xB3Og"
      },
      "outputs": [],
      "source": [
        "plt.bar(height = cantPublicaciones, x= etiquetas)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5H1oFdImIkV"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "plt.hist(rankPos.values(), bins = 453,color = \"darkcyan\")\n",
        "plt.xticks(np.arange(0,453,step=20))\n",
        "plt.ylabel(\"Usuarios\")\n",
        "plt.xlabel(\"Cantidad de entradas publicadas\")\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiyabHZXqAZf"
      },
      "outputs": [],
      "source": [
        "lista_de_entradas = [x for x in rankPos.values() if x < 50]\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.hist(lista_de_entradas, bins = 50,color = \"darkcyan\")\n",
        "plt.xticks(np.arange(0,50,step=1))\n",
        "plt.ylabel(\"Usuarios\")\n",
        "plt.xlabel(\"Cantidad de entradas publicadas\")\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iR9uUastrBF"
      },
      "outputs": [],
      "source": [
        "lista_de_entradas3 = [x for x in rankNeg.values() if x < 453]\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.hist(lista_de_entradas3, bins = 453,color = \"m\")\n",
        "plt.xticks(np.arange(0,453,step=20))\n",
        "plt.ylabel(\"Usuarios\")\n",
        "plt.xlabel(\"Cantidad de entradas publicadas\")\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQ2gEO5mvEEp"
      },
      "outputs": [],
      "source": [
        "lista_de_entradas2 = [x for x in rankNeg.values() if x < 50]\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.hist(lista_de_entradas2, bins = 50,color = \"m\")\n",
        "plt.xticks(np.arange(0,50,step=1))\n",
        "plt.ylabel(\"Usuarios\")\n",
        "plt.xlabel(\"Cantidad de entradas publicadas\")\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mTTSR5KCryn"
      },
      "source": [
        "##quiero analizar al 5% de los usuarios que publicaron mas del 50% de los posRank\n",
        "\n",
        "voy a hacer un promedio de todas sus publicaciones y ver si tienen mas publicaciones positivas que negativas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4HZ1K3xCrme"
      },
      "outputs": [],
      "source": [
        "\n",
        "promediar = np.zeros(2)\n",
        "for n in range(len(analizar5porciento)):\n",
        "  i = analizar5porciento[n]\n",
        "  if(i in rankPos):\n",
        "    promediar[0] += rankPos[i]\n",
        "  if(i in rankNeg):\n",
        "    promediar[1] += rankNeg[i]\n",
        "\n",
        "data = promediar / len(analizar5porciento)\n",
        "\n",
        "promediar1 = np.zeros(2)\n",
        "for n in range(len(analizar95porciento)):\n",
        "  i = analizar95porciento[n]\n",
        "  if(i in rankPos):\n",
        "    promediar1[0] += rankPos[i]\n",
        "  if(i in rankNeg):\n",
        "    promediar1[1] += rankNeg[i]\n",
        "\n",
        "data1 = promediar1 / len(analizar95porciento)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(4,8))\n",
        "plt.bar(height=promediar , x=[\"Positivo\",\"Negativo\"], color = [\"blue\",\"red\"], width=0.8)\n",
        "\n",
        "plt.show()\n",
        "print(promediar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snbCfZ6RNjjz"
      },
      "outputs": [],
      "source": [
        "promediar1 = np.zeros(2)\n",
        "for n in range(len(analizar95porciento)):\n",
        "  i = analizar95porciento[n]\n",
        "  if(i in rankPos):\n",
        "    promediar1[0] += rankPos[i]\n",
        "  if(i in rankNeg):\n",
        "    promediar1[1] += rankNeg[i]\n",
        "\n",
        "data1 = promediar1 / len(analizar95porciento)\n",
        "\n",
        "plt.figure(figsize=(4,8))\n",
        "plt.bar(height=promediar1 , x=[\"Positivo\",\"Negativo\"], color = [\"blue\",\"red\"])\n",
        "plt.show()\n",
        "print(promediar1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL6GoZVaIrRK"
      },
      "source": [
        "yo de aca concluyo que tienen muchos buenos rankeados pero porque escriben muchos-> esto me lleva a descartar una hipotesis de mentiras/hacerlo para obtener puntaje alto\n",
        "\n",
        "\n",
        "\n",
        "vemos que las sumas de los suenios negativos y los positivos nos dice que el 5% que tienen muchos bien rankeados es porque escriben mucho. mientras que el 95% esta mas parejo en cuanto al rank?? nose si meter esto /// creo q no"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q85oPt_xZa8S"
      },
      "source": [
        "['cohesion', 'intent', 'lucidity', 'raicesl', 'rating', 'technique', 'url', 'user', 'dream', 'additional_comments', 'themes', 'settings', 'characters', 'emotions', 'activities']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Quiero hacer porcentajes por cada usuario q publico alguna vez algun suenio para ver como suelen ser rankeados sus suenios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSCh1sGwZbHv"
      },
      "outputs": [],
      "source": [
        "\n",
        "dicPorcentajes = dict()\n",
        "rankIT = list(rank.keys())\n",
        "for i in range(len(rank.keys())):\n",
        "  key = rankIT[i]\n",
        "  if(key in rankPos):\n",
        "    pos = (rankPos[key]*100)/rank[key]\n",
        "    pos = round(pos,1)\n",
        "  else:\n",
        "    pos = 0\n",
        "  if(key in rankNeg):\n",
        "    neg = (rankNeg[key]*100)/rank[key]\n",
        "    neg = round(neg,1)\n",
        "  else:\n",
        "    neg = 0\n",
        "  neu = 100 -(pos+neg)\n",
        "  dicPorcentajes[key] = [pos,neg,neu]\n",
        "\n",
        "\n",
        "#busqueda\n",
        "busquedaPositivos = []\n",
        "it = list(dicPorcentajes.keys())\n",
        "for i in range(len(dicPorcentajes.keys())):\n",
        "  key = it[i]\n",
        "  #if(dicPorcentajes[key][0]>dicPorcentajes[key][1] and rank[key]>8):\n",
        "  if(dicPorcentajes[key][0]>dicPorcentajes[key][1] and rank[key]>8):\n",
        "    busquedaPositivos.append([key,dicPorcentajes[key][0],dicPorcentajes[key][1],rank[key]])\n",
        "\n",
        "#busquedaPositivos#[user,%pos,%neg,cantPublicaciones]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA8ZIX6LhQW7"
      },
      "source": [
        "de hecho existen usuarios con todos sus suenios rankeados positivos pero estos no superan los 10 suenios publicados"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "O32QYyzuxsa3",
        "CkGdGHdsyQwu",
        "KtcKmEbMRNMG",
        "i4C05KbvEOOP",
        "tHSE17nt1tv8",
        "MiFXIVocRQ_3"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
