{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhd-sJ-CqlpE"
      },
      "source": [
        "#0. Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nxuPRrvZrMrM"
      },
      "outputs": [],
      "source": [
        "# Importamos las librerías habituales\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O32QYyzuxsa3"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##Cómo levantar el df limpio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\pickle.py:202\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    201\u001b[0m         warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mWarning\u001b[39;00m)\n\u001b[1;32m--> 202\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m excs_to_catch:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;66;03m# e.g.\u001b[39;00m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;66;03m#  \"No module named 'pandas.core.sparse.series'\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;66;03m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:2728\u001b[0m, in \u001b[0;36mnew_block\u001b[1;34m(values, placement, ndim, refs)\u001b[0m\n\u001b[0;32m   2727\u001b[0m klass \u001b[38;5;241m=\u001b[39m get_block_type(values\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m-> 2728\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplacement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mTypeError\u001b[0m: Argument 'placement' has incorrect type (expected pandas._libs.internals.BlockPlacement, got slice)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m14\u001b[39m):\n\u001b[0;32m      7\u001b[0m   path \u001b[38;5;241m=\u001b[39m directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 8\u001b[0m   df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m   dataframes\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Concatenate all dataframes in the list\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\pickle.py:207\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m excs_to_catch:\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;66;03m# e.g.\u001b[39;00m\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;66;03m#  \"No module named 'pandas.core.sparse.series'\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;66;03m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[39;00m\n\u001b[1;32m--> 207\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;66;03m# e.g. can occur for files written in py27; see GH#28645 and GH#31988\u001b[39;00m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\compat\\pickle_compat.py:231\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fh, encoding, is_verbose)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# \"Unpickler\" has no attribute \"is_verbose\"  [attr-defined]\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     up\u001b[38;5;241m.\u001b[39mis_verbose \u001b[38;5;241m=\u001b[39m is_verbose  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\pickle.py:1205\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1203\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1205\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[0;32m   1207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
            "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\pickle.py:1232\u001b[0m, in \u001b[0;36m_Unpickler.load_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame_size \u001b[38;5;241m>\u001b[39m sys\u001b[38;5;241m.\u001b[39mmaxsize:\n\u001b[0;32m   1231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe size > sys.maxsize: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m frame_size)\n\u001b[1;32m-> 1232\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unframer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_size\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\pickle.py:311\u001b[0m, in \u001b[0;36m_Unframer.load_frame\u001b[1;34m(self, frame_size)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_frame \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_frame\u001b[38;5;241m.\u001b[39mread() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeginning of a new frame before end of current frame\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_frame \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_size\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "directory = 'Data/limp'\n",
        "dataframes = []  # list to hold all dataframes\n",
        "\n",
        "for i in range(1,14):\n",
        "  path = directory + str(i) + '.pickle'\n",
        "  df = pd.read_pickle(path)\n",
        "  dataframes.append(df)\n",
        "\n",
        "# Concatenate all dataframes in the list\n",
        "df = pd.concat(dataframes)\n",
        "dfRank = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "gHOkoltntvxZ"
      },
      "outputs": [],
      "source": [
        "df[\"raices\"] = df[\"raicesl\"]\n",
        "df[\"text\"] = df[\"dream\"]\n",
        "\n",
        "#Junta y arma un solo \"texto\" con las raíces\n",
        "df['raices_unidas'] = df['raices'].apply(\" \".join)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YH8ySeQx8x6"
      },
      "source": [
        "Es casi igual que antes, solo que con 13 archivos limp#.pickle  \n",
        "También cambiaron las columnas:  \n",
        "['cohesion', 'intent', 'lucidity', 'raicesl', 'rating', 'technique',\n",
        "       'url', 'user', 'dream', 'additional_comments', 'themes', 'settings',\n",
        "       'characters', 'emotions', 'activities']\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkGdGHdsyQwu"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "##Cómo levantar el df Viejo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gPrqamGVqnMp"
      },
      "outputs": [],
      "source": [
        "#Correr solo esto para tener un df de 9k entradas\n",
        "#directory = '/content/drive/My Drive/Dreams/data'\n",
        "#df = pd.read_pickle('/content/drive/My Drive/Dreams/data1.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "kIXc3X67rGYd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cohesion</th>\n",
              "      <th>intent</th>\n",
              "      <th>lucidity</th>\n",
              "      <th>raicesl</th>\n",
              "      <th>rating</th>\n",
              "      <th>technique</th>\n",
              "      <th>url</th>\n",
              "      <th>user</th>\n",
              "      <th>dream</th>\n",
              "      <th>additional_comments</th>\n",
              "      <th>themes</th>\n",
              "      <th>settings</th>\n",
              "      <th>characters</th>\n",
              "      <th>emotions</th>\n",
              "      <th>activities</th>\n",
              "      <th>raices</th>\n",
              "      <th>text</th>\n",
              "      <th>raices_unidas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10000</th>\n",
              "      <td>5.0</td>\n",
              "      <td>red</td>\n",
              "      <td>2.50</td>\n",
              "      <td>[this, dream, start, out, very, dramatic, and,...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>http://www.dreamjournal.net/journal/dream/drea...</td>\n",
              "      <td>WilliamDreams</td>\n",
              "      <td>this dream starts out very dramatic and then ...</td>\n",
              "      <td>i don t have any clue why i had this dream it ...</td>\n",
              "      <td>dramatic spiritual mystical aggression</td>\n",
              "      <td>outdoors indoors unfamiliar</td>\n",
              "      <td>stranger creatures</td>\n",
              "      <td>anger anxiety shock</td>\n",
              "      <td>physical thinking visual searching location ch...</td>\n",
              "      <td>[this, dream, start, out, very, dramatic, and,...</td>\n",
              "      <td>this dream starts out very dramatic and then ...</td>\n",
              "      <td>this dream start out very dramatic and then se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10001</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>[i, live, with, my, dad, in, an, underground, ...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>http://www.dreamjournal.net/journal/dream/drea...</td>\n",
              "      <td>Sakurama</td>\n",
              "      <td>i lived with my dad in an underground house t...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[i, live, with, my, dad, in, an, underground, ...</td>\n",
              "      <td>i lived with my dad in an underground house t...</td>\n",
              "      <td>i live with my dad in an underground house the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10002</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>[i, be, drive, through, a, downtown, area, tha...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>http://www.dreamjournal.net/journal/dream/drea...</td>\n",
              "      <td>haux</td>\n",
              "      <td>i was driving through a downtown area that re...</td>\n",
              "      <td></td>\n",
              "      <td>aggression</td>\n",
              "      <td>outdoors distorted familiar</td>\n",
              "      <td>imaginary</td>\n",
              "      <td>anger fear dread anxiety confusion</td>\n",
              "      <td>auditory physical thinking visual movement sea...</td>\n",
              "      <td>[i, be, drive, through, a, downtown, area, tha...</td>\n",
              "      <td>i was driving through a downtown area that re...</td>\n",
              "      <td>i be drive through a downtown area that remind...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10003</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>[the, three, main, political, party, be, have,...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>http://www.dreamjournal.net/journal/dream/drea...</td>\n",
              "      <td>london_lad</td>\n",
              "      <td>the three main political parties were having ...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[the, three, main, political, party, be, have,...</td>\n",
              "      <td>the three main political parties were having ...</td>\n",
              "      <td>the three main political party be have their p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10004</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>[i, have, this, dream, last, week, but, have, ...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>http://www.dreamjournal.net/journal/dream/drea...</td>\n",
              "      <td>Concord</td>\n",
              "      <td>i had this dream last week but have been too ...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[i, have, this, dream, last, week, but, have, ...</td>\n",
              "      <td>i had this dream last week but have been too ...</td>\n",
              "      <td>i have this dream last week but have be too bu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>[i, be, travel, from, a, to, b, and, be, be, f...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>http://www.dreamjournal.net/journal/dream/drea...</td>\n",
              "      <td>london_lad</td>\n",
              "      <td>i was traveling from a to b and was being fol...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[i, be, travel, from, a, to, b, and, be, be, f...</td>\n",
              "      <td>i was traveling from a to b and was being fol...</td>\n",
              "      <td>i be travel from a to b and be be follow by an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>1.0</td>\n",
              "      <td>red</td>\n",
              "      <td>2.50</td>\n",
              "      <td>[this, morning, i, wake, up, briefly, and, whe...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>http://www.dreamjournal.net/journal/dream/drea...</td>\n",
              "      <td>BitterOlives</td>\n",
              "      <td>this morning i woke up briefly and when i sh...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>outdoors geographical unfamiliar</td>\n",
              "      <td></td>\n",
              "      <td>shock</td>\n",
              "      <td>thinking visual</td>\n",
              "      <td>[this, morning, i, wake, up, briefly, and, whe...</td>\n",
              "      <td>this morning i woke up briefly and when i sh...</td>\n",
              "      <td>this morning i wake up briefly and when i shut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>2.0</td>\n",
              "      <td>red</td>\n",
              "      <td>1.25</td>\n",
              "      <td>[i, be, drive, around, fayetteville, ar, and, ...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>http://www.dreamjournal.net/journal/dream/drea...</td>\n",
              "      <td>BitterOlives</td>\n",
              "      <td>i was driving around fayetteville ar and sup...</td>\n",
              "      <td></td>\n",
              "      <td>fun action authority relationships friendly</td>\n",
              "      <td>town city outdoors indoors automobile distorte...</td>\n",
              "      <td>child significant other friend stranger unfami...</td>\n",
              "      <td>worry anxiety happiness relaxed confusion</td>\n",
              "      <td>auditory physical thinking visual movement dri...</td>\n",
              "      <td>[i, be, drive, around, fayetteville, ar, and, ...</td>\n",
              "      <td>i was driving around fayetteville ar and sup...</td>\n",
              "      <td>i be drive around fayetteville ar and supposed...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>[it, be, a, smattering, of, wierdness, we, be,...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>http://www.dreamjournal.net/journal/dream/drea...</td>\n",
              "      <td>brainliquid</td>\n",
              "      <td>it was a smattering of wierdness we were trav...</td>\n",
              "      <td></td>\n",
              "      <td>sexuality action aggression</td>\n",
              "      <td>outdoors distorted</td>\n",
              "      <td>unfamiliar</td>\n",
              "      <td>confusion</td>\n",
              "      <td>physical movement</td>\n",
              "      <td>[it, be, a, smattering, of, wierdness, we, be,...</td>\n",
              "      <td>it was a smattering of wierdness we were trav...</td>\n",
              "      <td>it be a smattering of wierdness we be travel a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>[i, be, in, this, ghetto, look, apartment, bui...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>http://www.dreamjournal.net/journal/dream/drea...</td>\n",
              "      <td>Peri</td>\n",
              "      <td>i was in this ghetto looking apartment buildi...</td>\n",
              "      <td>a ghetto can represent feelings of being witho...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>and not resisting or considering what you can ...</td>\n",
              "      <td></td>\n",
              "      <td>[i, be, in, this, ghetto, look, apartment, bui...</td>\n",
              "      <td>i was in this ghetto looking apartment buildi...</td>\n",
              "      <td>i be in this ghetto look apartment building i ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 18 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       cohesion intent  lucidity  \\\n",
              "10000       5.0    red      2.50   \n",
              "10001       1.0     -1     -1.00   \n",
              "10002       2.0     -1     -1.00   \n",
              "10003       1.0     -1     -1.00   \n",
              "10004       1.0     -1     -1.00   \n",
              "...         ...    ...       ...   \n",
              "19995       1.0     -1     -1.00   \n",
              "19996       1.0    red      2.50   \n",
              "19997       2.0    red      1.25   \n",
              "19998       1.0     -1     -1.00   \n",
              "19999       1.0     -1     -1.00   \n",
              "\n",
              "                                                 raicesl  rating technique  \\\n",
              "10000  [this, dream, start, out, very, dramatic, and,...     3.0        -1   \n",
              "10001  [i, live, with, my, dad, in, an, underground, ...    -1.0        -1   \n",
              "10002  [i, be, drive, through, a, downtown, area, tha...     3.0        -1   \n",
              "10003  [the, three, main, political, party, be, have,...    -1.0        -1   \n",
              "10004  [i, have, this, dream, last, week, but, have, ...    -1.0        -1   \n",
              "...                                                  ...     ...       ...   \n",
              "19995  [i, be, travel, from, a, to, b, and, be, be, f...    -1.0        -1   \n",
              "19996  [this, morning, i, wake, up, briefly, and, whe...     5.0        -1   \n",
              "19997  [i, be, drive, around, fayetteville, ar, and, ...     3.0        -1   \n",
              "19998  [it, be, a, smattering, of, wierdness, we, be,...     1.0        -1   \n",
              "19999  [i, be, in, this, ghetto, look, apartment, bui...     2.0        -1   \n",
              "\n",
              "                                                     url           user  \\\n",
              "10000  http://www.dreamjournal.net/journal/dream/drea...  WilliamDreams   \n",
              "10001  http://www.dreamjournal.net/journal/dream/drea...       Sakurama   \n",
              "10002  http://www.dreamjournal.net/journal/dream/drea...           haux   \n",
              "10003  http://www.dreamjournal.net/journal/dream/drea...     london_lad   \n",
              "10004  http://www.dreamjournal.net/journal/dream/drea...        Concord   \n",
              "...                                                  ...            ...   \n",
              "19995  http://www.dreamjournal.net/journal/dream/drea...     london_lad   \n",
              "19996  http://www.dreamjournal.net/journal/dream/drea...   BitterOlives   \n",
              "19997  http://www.dreamjournal.net/journal/dream/drea...   BitterOlives   \n",
              "19998  http://www.dreamjournal.net/journal/dream/drea...    brainliquid   \n",
              "19999  http://www.dreamjournal.net/journal/dream/drea...           Peri   \n",
              "\n",
              "                                                   dream  \\\n",
              "10000   this dream starts out very dramatic and then ...   \n",
              "10001   i lived with my dad in an underground house t...   \n",
              "10002   i was driving through a downtown area that re...   \n",
              "10003   the three main political parties were having ...   \n",
              "10004   i had this dream last week but have been too ...   \n",
              "...                                                  ...   \n",
              "19995   i was traveling from a to b and was being fol...   \n",
              "19996    this morning i woke up briefly and when i sh...   \n",
              "19997    i was driving around fayetteville ar and sup...   \n",
              "19998   it was a smattering of wierdness we were trav...   \n",
              "19999   i was in this ghetto looking apartment buildi...   \n",
              "\n",
              "                                     additional_comments  \\\n",
              "10000  i don t have any clue why i had this dream it ...   \n",
              "10001                                                      \n",
              "10002                                                      \n",
              "10003                                                      \n",
              "10004                                                      \n",
              "...                                                  ...   \n",
              "19995                                                      \n",
              "19996                                                      \n",
              "19997                                                      \n",
              "19998                                                      \n",
              "19999  a ghetto can represent feelings of being witho...   \n",
              "\n",
              "                                             themes  \\\n",
              "10000       dramatic spiritual mystical aggression    \n",
              "10001                                                 \n",
              "10002                                   aggression    \n",
              "10003                                                 \n",
              "10004                                                 \n",
              "...                                             ...   \n",
              "19995                                                 \n",
              "19996                                                 \n",
              "19997  fun action authority relationships friendly    \n",
              "19998                  sexuality action aggression    \n",
              "19999                                                 \n",
              "\n",
              "                                                settings  \\\n",
              "10000                       outdoors indoors unfamiliar    \n",
              "10001                                                      \n",
              "10002                       outdoors distorted familiar    \n",
              "10003                                                      \n",
              "10004                                                      \n",
              "...                                                  ...   \n",
              "19995                                                      \n",
              "19996                  outdoors geographical unfamiliar    \n",
              "19997  town city outdoors indoors automobile distorte...   \n",
              "19998                                outdoors distorted    \n",
              "19999                                                      \n",
              "\n",
              "                                              characters  \\\n",
              "10000                                stranger creatures    \n",
              "10001                                                      \n",
              "10002                                         imaginary    \n",
              "10003                                                      \n",
              "10004                                                      \n",
              "...                                                  ...   \n",
              "19995                                                      \n",
              "19996                                                      \n",
              "19997  child significant other friend stranger unfami...   \n",
              "19998                                        unfamiliar    \n",
              "19999                                                      \n",
              "\n",
              "                                                emotions  \\\n",
              "10000                               anger anxiety shock    \n",
              "10001                                                      \n",
              "10002                anger fear dread anxiety confusion    \n",
              "10003                                                      \n",
              "10004                                                      \n",
              "...                                                  ...   \n",
              "19995                                                      \n",
              "19996                                             shock    \n",
              "19997         worry anxiety happiness relaxed confusion    \n",
              "19998                                         confusion    \n",
              "19999  and not resisting or considering what you can ...   \n",
              "\n",
              "                                              activities  \\\n",
              "10000  physical thinking visual searching location ch...   \n",
              "10001                                                      \n",
              "10002  auditory physical thinking visual movement sea...   \n",
              "10003                                                      \n",
              "10004                                                      \n",
              "...                                                  ...   \n",
              "19995                                                      \n",
              "19996                                   thinking visual    \n",
              "19997  auditory physical thinking visual movement dri...   \n",
              "19998                                 physical movement    \n",
              "19999                                                      \n",
              "\n",
              "                                                  raices  \\\n",
              "10000  [this, dream, start, out, very, dramatic, and,...   \n",
              "10001  [i, live, with, my, dad, in, an, underground, ...   \n",
              "10002  [i, be, drive, through, a, downtown, area, tha...   \n",
              "10003  [the, three, main, political, party, be, have,...   \n",
              "10004  [i, have, this, dream, last, week, but, have, ...   \n",
              "...                                                  ...   \n",
              "19995  [i, be, travel, from, a, to, b, and, be, be, f...   \n",
              "19996  [this, morning, i, wake, up, briefly, and, whe...   \n",
              "19997  [i, be, drive, around, fayetteville, ar, and, ...   \n",
              "19998  [it, be, a, smattering, of, wierdness, we, be,...   \n",
              "19999  [i, be, in, this, ghetto, look, apartment, bui...   \n",
              "\n",
              "                                                    text  \\\n",
              "10000   this dream starts out very dramatic and then ...   \n",
              "10001   i lived with my dad in an underground house t...   \n",
              "10002   i was driving through a downtown area that re...   \n",
              "10003   the three main political parties were having ...   \n",
              "10004   i had this dream last week but have been too ...   \n",
              "...                                                  ...   \n",
              "19995   i was traveling from a to b and was being fol...   \n",
              "19996    this morning i woke up briefly and when i sh...   \n",
              "19997    i was driving around fayetteville ar and sup...   \n",
              "19998   it was a smattering of wierdness we were trav...   \n",
              "19999   i was in this ghetto looking apartment buildi...   \n",
              "\n",
              "                                           raices_unidas  \n",
              "10000  this dream start out very dramatic and then se...  \n",
              "10001  i live with my dad in an underground house the...  \n",
              "10002  i be drive through a downtown area that remind...  \n",
              "10003  the three main political party be have their p...  \n",
              "10004  i have this dream last week but have be too bu...  \n",
              "...                                                  ...  \n",
              "19995  i be travel from a to b and be be follow by an...  \n",
              "19996  this morning i wake up briefly and when i shut...  \n",
              "19997  i be drive around fayetteville ar and supposed...  \n",
              "19998  it be a smattering of wierdness we be travel a...  \n",
              "19999  i be in this ghetto look apartment building i ...  \n",
              "\n",
              "[10000 rows x 18 columns]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Todos los pickles (ESTÁ PUESTO HASTA EL 5: TODOS ES HASTA EL 15)\n",
        "#for i in range(2,5):\n",
        "#  path = directory + str(i) + '.pickle'\n",
        "#  df1 = pd.read_pickle(path)\n",
        "#  df = df.append(df1, ignore_index=True)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuxmk0zpt5Lu"
      },
      "source": [
        "#1. Limpieza previa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "GfCbjX9z8G2T"
      },
      "outputs": [],
      "source": [
        "#Hay algunos repetidos...\n",
        "print(df[\"raices\"][35427])\n",
        "print(df[\"raices\"][35428])\n",
        "print(df[\"url\"][35427])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnwNfoS0t_H3"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVZ16bftQmwW"
      },
      "source": [
        "# 2. ¿Qué sueña la gente?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtcKmEbMRNMG"
      },
      "source": [
        "#Análisis de sentimientos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amdUXFVfQ5Mc"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "#no sé si usamos todo esto\n",
        "nltk.download([\n",
        "     \"names\",\n",
        "     \"stopwords\",\n",
        "     \"state_union\",\n",
        "     \"twitter_samples\",\n",
        "     \"movie_reviews\",\n",
        "     \"averaged_perceptron_tagger\",\n",
        "     \"vader_lexicon\",\n",
        "     \"punkt\",\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zS_RyKmcSMsj"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "df[\"sentiment\"] = df[\"text\"].apply(sia.polarity_scores)   #TARDA BASTANTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI2uH3JDhuAG"
      },
      "outputs": [],
      "source": [
        "df[\"sentiment\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ww1RCcHzhxDY"
      },
      "outputs": [],
      "source": [
        "df[\"neg\"] = pd.Series\n",
        "df[\"pos\"] = pd.Series\n",
        "\n",
        "for i in df[\"sentiment\"].index:\n",
        "  df[\"neg\"][i] = df[\"sentiment\"][i][\"neg\"]\n",
        "  df[\"pos\"][i] = df[\"sentiment\"][i][\"pos\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VuiYa0DtonPY"
      },
      "outputs": [],
      "source": [
        "#Los que tienen más de 0,5 en positivo\n",
        "df[df[\"pos\"]>0.5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LbjaaIgFsL9p"
      },
      "outputs": [],
      "source": [
        "#Más de 0,7 en negativo\n",
        "df[df[\"neg\"]>0.7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "03PJKvy5r9mz"
      },
      "outputs": [],
      "source": [
        "#Histograma negativo\n",
        "df[\"neg\"].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4C05KbvEOOP"
      },
      "source": [
        "# Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "m9U19bL7ELBB"
      },
      "outputs": [],
      "source": [
        "from gensim.models.phrases import Phrases, Phraser\n",
        "\n",
        "input = [row.split() for row in df['raices_unidas']] # separamos en una lista\n",
        "\n",
        "phrases = Phrases(input, min_count=30, progress_per=1000)\n",
        "\n",
        "bigram = Phraser(phrases)\n",
        "\n",
        "sentences = bigram[input] # obtenemos las palabras junto con bigramas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TfxgpxyFEWfn",
        "outputId": "fa75124a-f51e-458b-8d22-35213df17ad4"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-bac3b0c981a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m### ENTRENA EL MODELO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m### PRECOMPUTA DISTANCIAS (mas rapido)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import multiprocessing\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "cores = multiprocessing.cpu_count()\n",
        "\n",
        "w2v_model = Word2Vec(min_count=20, # ignora palabras cuya frecuencia es menor a esta\n",
        "                     window=2, # tamanio de la ventana de contexto\n",
        "                     size=300, # dimension del embedding\n",
        "                     sample=6e-5, # umbral para downsamplear palabras muy frecuentes\n",
        "                     alpha=0.03, # tasa de aprendizaje inicial (entrenamiento de la red neuronal)\n",
        "                     min_alpha=0.0007, # tasa de aprendizaje minima\n",
        "                     negative=20, # penalidad de palabras muy frecuentes o poco informaitvas\n",
        "                     workers=cores) # numero de cores para entrenar el modelo\n",
        "\n",
        "w2v_model.build_vocab(sentences, progress_per=10000) # construye el vocabulario\n",
        "\n",
        "### ENTRENA EL MODELO\n",
        "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
        "### PRECOMPUTA DISTANCIAS (mas rapido)\n",
        "w2v_model.init_sims(replace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5oubl29FIW3"
      },
      "outputs": [],
      "source": [
        "w2v_model.wv.most_similar(positive=[\"sex\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHSE17nt1tv8"
      },
      "source": [
        "# Tópicos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2ozjzgX1r8X"
      },
      "outputs": [],
      "source": [
        "# Importamos las librerías habituales\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY8U24471xvK"
      },
      "outputs": [],
      "source": [
        "# Objetos de sklearn para hacer tópicos\n",
        "from sklearn.feature_extraction.text import CountVectorizer # Contador de frecuencia\n",
        "from sklearn.feature_extraction.text import TfidfTransformer # Creador de tf-idf\n",
        "\n",
        "# Algoritmos de descomposición de tópicos\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8lauNvq10bC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Creamos el objeto contador de palabras, pidiéndole que remueve\n",
        "# las stopwords, los términos que aparecen en un único documento (min_df)\n",
        "# y los términos que aparecen en más del 70% de los documentos (max_df).\n",
        "# Esto es para eliminar palabras raras (o errores de tipeo) y\n",
        "# términos que seguramente son stopwords no incluídos en la lista\n",
        "count = CountVectorizer(min_df = 2, max_df = 0.70, stop_words = stopwords)\n",
        "\n",
        "# Ajustamos con los datos. Acá especificamente creamos una matriz documentos-términos\n",
        "x_count = count.fit_transform(df['raices_unidas'])\n",
        "\n",
        "# Dimensions de la matriz doc-tér\n",
        "print(x_count.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMx8OmJE12AG"
      },
      "outputs": [],
      "source": [
        "# Creamos el objeto tf-idf. Le decimos además que devuelva los\n",
        "# vectores documento con norma euclídea igual a 1 (norm = 'l2')\n",
        "tfidf = TfidfTransformer(norm = 'l2')\n",
        "\n",
        "# Creamos la matriz tf-idf a partir de la matriz de frecuencias\n",
        "x_tfidf = tfidf.fit_transform(x_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wtm6HqrD13lI"
      },
      "outputs": [],
      "source": [
        "# Elijamos la cantidad de tópicos\n",
        "n_components = 5\n",
        "\n",
        "# Construímos el objeto NMF con los tópicos indicados\n",
        "nmf = NMF(n_components = n_components)\n",
        "\n",
        "# Aplicamos sobre nuestros datos\n",
        "x_nmf = nmf.fit_transform(x_tfidf)\n",
        "\n",
        "# Dimensión de la matriz transformada\n",
        "print(x_nmf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiosTeSe15Js"
      },
      "outputs": [],
      "source": [
        "# Objeto índice: término de nuestro vocabulario\n",
        "vocabulary = {item: key for key, item in count.vocabulary_.items()}\n",
        "\n",
        "# Para cada componente\n",
        "for n in range(n_components):\n",
        "\n",
        "  # Ordenamos una lista del largo de nuestro vocabulario según el peso en cada componente y nos quedamos con los primeros 10\n",
        "  list_sorted = sorted(range(nmf.components_.shape[1]), reverse = True, key = lambda x: nmf.components_[n][x])[:10]\n",
        "\n",
        "  # Printeamos los términos asociados a los valores más grande de cada una de las componentes\n",
        "  print(', '.join([vocabulary[i] for i in list_sorted]))\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mAf0IT52_2z"
      },
      "outputs": [],
      "source": [
        "# WordClouds\n",
        "wc_atributos = {'height' : 800,\n",
        "                'width' : 1200,\n",
        "                'background_color' : 'white',\n",
        "                'max_words' : 20\n",
        "                } # Defino los parámetros que les voy a pasar a los wordclouds\n",
        "\n",
        "# Creo la figura\n",
        "fig, axs = plt.subplots(n_components, figsize = (6,20))\n",
        "\n",
        "# Recorro para todas las componentes\n",
        "for n in range(n_components):\n",
        "\n",
        "  # 10 términos más pesados\n",
        "  list_sorted = sorted(range(len(vocabulary)), reverse = True, key = lambda x: nmf.components_[n][x])[:10]\n",
        "\n",
        "  # Diccionario término: peso\n",
        "  comp_dict = {vocabulary[i]: nmf.components_[n][i] for i in list_sorted}\n",
        "\n",
        "  # Creo el wordlcoud\n",
        "  wc = WordCloud(**wc_atributos # De esta forma, le estoy diciendo a la función que expanda el diccionario de atributos de forma tal de que entienda lo que quiero que haga\n",
        "                 ).generate_from_frequencies(comp_dict)\n",
        "\n",
        "  axs[n].set_title('Tópico {}'.format(n))\n",
        "  axs[n].imshow(wc)\n",
        "  axs[n].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP_cQ86e66_L"
      },
      "outputs": [],
      "source": [
        "# Normalizador\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# Creamos un objeto para normalizar a que la suma dé 1\n",
        "norm = Normalizer('l1')\n",
        "\n",
        "# Sobreescribimos sobre la matriz de documentos-tópicos\n",
        "x_nmf = norm.fit_transform(x_nmf)\n",
        "\n",
        "# Guardemos en el dataframe esta información\n",
        "for n in range(n_components):\n",
        "  df['nmf_comp{}'.format(n)] = x_nmf[:,n]\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPbKrfPS69D6"
      },
      "outputs": [],
      "source": [
        "df_metodo = df.groupby('technique').mean()\n",
        "\n",
        "# Inspeccionemoslo\n",
        "df_metodo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQhYfa_I6_e0"
      },
      "outputs": [],
      "source": [
        "# El eje x es la década\n",
        "x = df_metodo.index\n",
        "\n",
        "# El eje y son las distribuciones\n",
        "y = df_metodo[['nmf_comp{}'.format(i) for i in range(n_components)]].to_numpy()\n",
        "\n",
        "plt.figure(figsize = (8,5))\n",
        "plt.stackplot(x, y.T) # Stackplot: sirve para graficar distribuciones\n",
        "#plt.xlim([0, 90])\n",
        "plt.ylim([0, 1.00])\n",
        "plt.yticks([])\n",
        "plt.xlabel('Método')\n",
        "plt.legend(['Tópico {}'.format(i) for i in range(n_components)], loc = (1.05, 0.60))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiFXIVocRQ_3"
      },
      "source": [
        "# 3. ¿Cómo sueña?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kV1WjPQF_3I"
      },
      "outputs": [],
      "source": [
        "#Nos quedamos solo con los que contestaron green o red\n",
        "index_red = df[\"intent\"]==\"red\"\n",
        "index_green = df[\"intent\"]==\"green\"\n",
        "\n",
        "index_or = index_green + index_red\n",
        "df = df[index_or]\n",
        "\n",
        "#Tabla de contingencia de lucidity e intent\n",
        "pd.crosstab(df[\"intent\"], df[\"lucidity\"], margins=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czbpNg1h6cwK"
      },
      "outputs": [],
      "source": [
        "#Qué técnicas usaron\n",
        "df[\"technique\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1zhuRXuEqy0"
      },
      "outputs": [],
      "source": [
        "#Tabla de contingencia de técnica e intent\n",
        "pd.crosstab(df[\"intent\"], df[\"technique\"], margins = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7s2I_OnEsWx"
      },
      "outputs": [],
      "source": [
        "#Tabla de contingencia de lucidity y técnica\n",
        "pd.crosstab(df[\"lucidity\"], df[\"technique\"], margins = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sJ37GPrtLGl"
      },
      "source": [
        "#Nubes de palabras por intention: green/red"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCzldZNY8sQB"
      },
      "outputs": [],
      "source": [
        "#Me quedo con las raíces\n",
        "raices = df[\"raices_unidas\"].transpose()\n",
        "raices_green = df[\"raices_unidas\"][df[\"intent\"]==\"green\"].transpose()\n",
        "raices_red = df[\"raices_unidas\"][df[\"intent\"]==\"red\"].transpose()\n",
        "\n",
        "#Las junto todos en un solo texto de raices\n",
        "todos_textos_green = \" \".join(raices_green)\n",
        "todos_textos_red = \" \".join(raices_red)\n",
        "\n",
        "#Las cuento\n",
        "print(\"Hay en total:\", raices.size, \"relatos\")  #hay vacíos?\n",
        "print(\"De los cuales son red:\", raices_red.size)\n",
        "print(\"La cant de green son:\", raices_green.size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jSMhiPwb0Uj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Cargamos del paquete nltk las stopwords a la lista \"stopwords\"\n",
        "import nltk\n",
        "nltk.download('stopwords') # hay que descargar este modulo en particular\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "print(stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez-PO0hZjEE8"
      },
      "outputs": [],
      "source": [
        "#Limpieza adicional para este set\n",
        "stopwords = stopwords + [\"quot\", \"nbsp\", \"additional\", \"comment\", \"rsquo\", \"rdquo\", \"unfamiliar character\"] #add emotions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI4v_Jh3BoE2"
      },
      "outputs": [],
      "source": [
        "print(df[\"raices\"][151])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfXz1KRD8v6N"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud # importo la funcion WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Creo el objeto WordCloud sacando la lista de stopwords\n",
        "wc = WordCloud(stopwords=stopwords, background_color=\"white\", colormap=\"Dark2\",\n",
        "               max_font_size=150, random_state=42)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [16,12] # tamaño de los plots\n",
        "\n",
        "#Genero green\n",
        "wc.generate(todos_textos_green)\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Green sin stopwords\")\n",
        "plt.show()\n",
        "\n",
        "#Genero red\n",
        "wc.generate(todos_textos_red)\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Red sin stopwords\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KTtwrenzMhQ"
      },
      "outputs": [],
      "source": [
        "# Ahora con stopwords\n",
        "wc = WordCloud(background_color=\"white\", colormap=\"Dark2\",\n",
        "               max_font_size=150, random_state=42)\n",
        "\n",
        "wc.generate(todos_textos_green)  # acá le pido que genere los WC a partir del texto de cada año\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Green con stopwords\")\n",
        "plt.show()\n",
        "\n",
        "wc.generate(todos_textos_red)  # acá le pido que genere los WC a partir del texto de cada año\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Red con stopwords\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJDPE6GMoeiA"
      },
      "outputs": [],
      "source": [
        "#Armo listas de raices green/red con y sin stopwords:\n",
        "#Primero con stopwords:\n",
        "lista_raices_green = df[\"raices\"][df[\"intent\"]==\"green\"].transpose()\n",
        "lista_raices_red = df[\"raices\"][df[\"intent\"]==\"red\"].transpose()\n",
        "\n",
        "#Función que remueve de una lista de strings los elementos en stopwords\n",
        "def sw_remover3(lista):\n",
        "   return [value for value in lista if not value in stopwords]\n",
        "\n",
        "#Aplico la función y lo guardo en una nueva columna\n",
        "lista_raices_green_sinsw = lista_raices_green.apply(sw_remover3)\n",
        "lista_raices_red_sinsw = lista_raices_red.apply(sw_remover3)\n",
        "\n",
        "# Se borraron los stopwords, por ejemplo:\n",
        "print(lista_raices_red[4])\n",
        "print(lista_raices_red_sinsw[4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL8YOGcCxJZm"
      },
      "source": [
        "Sobre el dataset:  \n",
        "-1 significa que no había datos al scrapear  \n",
        "No hay Nan entre los elementos.  \n",
        "Hay ruido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTzj_zfKzZw-"
      },
      "outputs": [],
      "source": [
        "# Importamos las librerías habituales\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsntAzeVzhd4"
      },
      "outputs": [],
      "source": [
        "# Objetos de sklearn para hacer tópicos\n",
        "from sklearn.feature_extraction.text import CountVectorizer # Contador de frecuencia\n",
        "from sklearn.feature_extraction.text import TfidfTransformer # Creador de tf-idf\n",
        "\n",
        "# Algoritmos de descomposición de tópicos\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4xPEhPffkv8"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftEs0t_NfonF"
      },
      "source": [
        "#RANKEO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRoRsAs-fr4_"
      },
      "outputs": [],
      "source": [
        "\n",
        "ratings = dfRank[\"rating\"].value_counts().to_dict()\n",
        "\n",
        "print(\"Total de sueños no rankeados = \" + str(ratings[-1]))\n",
        "print(\"Total de sueños rankeados = \" + str(ratings[5]+ratings[4]+ratings[3]+ratings[2]+ratings[1]))\n",
        "print(\"Total de sueños rankeados positivos = \" + str(ratings[5]+ratings[4]))\n",
        "print(\"Total de sueños rankeados negativos = \" + str(ratings[2]+ratings[1]))\n",
        "print(\"Total de sueños rankeados neutros = \"+str(ratings[3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20ogg03eZQk6"
      },
      "source": [
        "Creo los dataframes segun rankeo positivo o negativo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GC6XZQSiWQ_Y"
      },
      "outputs": [],
      "source": [
        "dfRank = dfRank.loc[dfRank[\"rating\"]>0] #elimino los suenios no rankeados == -1\n",
        "dfPosRank = dfRank.loc[dfRank[\"rating\"]>3] #asumo rank pos == mayor a 3\n",
        "dfNegRank = dfRank.loc[dfRank[\"rating\"]<3] #asumo rank neg == menor a 3\n",
        "\n",
        "rank = dfRank[\"user\"].value_counts().to_dict()\n",
        "rankPos = dfPosRank[\"user\"].value_counts().to_dict()\n",
        "rankNeg = dfNegRank[\"user\"].value_counts().to_dict()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzHOfQ9sQIY-"
      },
      "source": [
        "##Son los suenios rankeados altos de los mismos usuarios?\n",
        "SI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZ9v_XtAQGeN"
      },
      "outputs": [],
      "source": [
        "data = list(rankPos.values())\n",
        "\n",
        "plt.pie(data, labels= rankPos.keys())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgHoc0zyWihk"
      },
      "source": [
        "## quiero ver cuantos usuarios escribieron que porcentaje del total\n",
        " es decir, se que aproximadamente 30 usuarios escribieron el 50% del total de los rankeados positivos, 200 usuarios el 5% y 8000 usuarios el otro 45%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4cdPaVUWu4T"
      },
      "outputs": [],
      "source": [
        "cantUsuarios = np.zeros(5)\n",
        "cantPublicaciones = np.zeros(5)\n",
        "etiquetas = [\"1-10\",\"11-30\",\"31-50\",\"51-100\",\"101-453\"]\n",
        "\n",
        "\n",
        "keys = list(rankPos.keys())\n",
        "analizar5porciento = []\n",
        "analizar95porciento = []\n",
        "\n",
        "\n",
        "for i in range(len(rankPos.keys())):\n",
        "  key = keys[i]\n",
        "  if(rankPos[key] < 11):\n",
        "    cantPublicaciones[0] += rankPos[key]\n",
        "    cantUsuarios[0] += 1\n",
        "    #para analisis del otro parte\n",
        "    analizar95porciento.append(key)\n",
        "  elif(rankPos[key] < 31):\n",
        "    cantPublicaciones[1] += rankPos[key]\n",
        "    cantUsuarios[1] += 1\n",
        "    #para mi grafico d analisis ams adelante\n",
        "    analizar5porciento.append(key)\n",
        "  elif(rankPos[key] < 51):\n",
        "    cantPublicaciones[2] += rankPos[key]\n",
        "    cantUsuarios[2] += 1\n",
        "    #para mi grafico d analisis ams adelante\n",
        "    analizar5porciento.append(key)\n",
        "  elif(rankPos[key] < 101):\n",
        "    cantPublicaciones[3] += rankPos[key]\n",
        "    cantUsuarios[3] += 1\n",
        "    #para mi grafico d analisis ams adelante\n",
        "    analizar5porciento.append(key)\n",
        "  else:\n",
        "    cantPublicaciones[4] += rankPos[key]\n",
        "    cantUsuarios[4] += 1\n",
        "    #para mi grafico d analisis ams adelante\n",
        "    analizar5porciento.append(key)\n",
        "\n",
        "\n",
        "for i in range(len(cantUsuarios)):\n",
        "  cantUsuarios[i] = str(cantUsuarios[i])\n",
        "\n",
        "\n",
        "print(cantPublicaciones,cantUsuarios)\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(nrows = 1, ncols = 2)\n",
        "axs[0].set_title(\"Suenios\")\n",
        "axs[0].pie(cantPublicaciones, labels = cantUsuarios ,colors = plt.get_cmap('Set3').colors, autopct='%1.2f%%')\n",
        "#axs[1].set_title[\"Usuarios\"]\n",
        "axs[1].pie(cantUsuarios, labels = etiquetas ,colors = plt.get_cmap('Set3').colors, autopct='%1.2f%%')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKns7kaXpGbp"
      },
      "source": [
        "grafico 1: porcentaje de entradas y usuarios que las escribieron/// suma de publicaciones que hicieron los usuarios entre rango\n",
        "\n",
        "grafico 2: usuarios sobre entradas que publicaron\n",
        "esto me lleva a querer analizar a este 5% de los usuarios///cantidad de  usuarios con x entradas entre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX-vLt9xB3Og"
      },
      "outputs": [],
      "source": [
        "plt.bar(height = cantPublicaciones, x= etiquetas)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5H1oFdImIkV"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "plt.hist(rankPos.values(), bins = 453,color = \"darkcyan\")\n",
        "plt.xticks(np.arange(0,453,step=20))\n",
        "plt.ylabel(\"Usuarios\")\n",
        "plt.xlabel(\"Cantidad de entradas publicadas\")\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiyabHZXqAZf"
      },
      "outputs": [],
      "source": [
        "lista_de_entradas = [x for x in rankPos.values() if x < 50]\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.hist(lista_de_entradas, bins = 50,color = \"darkcyan\")\n",
        "plt.xticks(np.arange(0,50,step=1))\n",
        "plt.ylabel(\"Usuarios\")\n",
        "plt.xlabel(\"Cantidad de entradas publicadas\")\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iR9uUastrBF"
      },
      "outputs": [],
      "source": [
        "lista_de_entradas3 = [x for x in rankNeg.values() if x < 453]\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.hist(lista_de_entradas3, bins = 453,color = \"m\")\n",
        "plt.xticks(np.arange(0,453,step=20))\n",
        "plt.ylabel(\"Usuarios\")\n",
        "plt.xlabel(\"Cantidad de entradas publicadas\")\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQ2gEO5mvEEp"
      },
      "outputs": [],
      "source": [
        "lista_de_entradas2 = [x for x in rankNeg.values() if x < 50]\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.hist(lista_de_entradas2, bins = 50,color = \"m\")\n",
        "plt.xticks(np.arange(0,50,step=1))\n",
        "plt.ylabel(\"Usuarios\")\n",
        "plt.xlabel(\"Cantidad de entradas publicadas\")\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mTTSR5KCryn"
      },
      "source": [
        "##quiero analizar al 5% de los usuarios que publicaron mas del 50% de los posRank\n",
        "\n",
        "voy a hacer un promedio de todas sus publicaciones y ver si tienen mas publicaciones positivas que negativas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4HZ1K3xCrme"
      },
      "outputs": [],
      "source": [
        "\n",
        "promediar = np.zeros(2)\n",
        "for n in range(len(analizar5porciento)):\n",
        "  i = analizar5porciento[n]\n",
        "  if(i in rankPos):\n",
        "    promediar[0] += rankPos[i]\n",
        "  if(i in rankNeg):\n",
        "    promediar[1] += rankNeg[i]\n",
        "\n",
        "data = promediar / len(analizar5porciento)\n",
        "\n",
        "promediar1 = np.zeros(2)\n",
        "for n in range(len(analizar95porciento)):\n",
        "  i = analizar95porciento[n]\n",
        "  if(i in rankPos):\n",
        "    promediar1[0] += rankPos[i]\n",
        "  if(i in rankNeg):\n",
        "    promediar1[1] += rankNeg[i]\n",
        "\n",
        "data1 = promediar1 / len(analizar95porciento)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(4,8))\n",
        "plt.bar(height=promediar , x=[\"Positivo\",\"Negativo\"], color = [\"blue\",\"red\"], width=0.8)\n",
        "\n",
        "plt.show()\n",
        "print(promediar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snbCfZ6RNjjz"
      },
      "outputs": [],
      "source": [
        "promediar1 = np.zeros(2)\n",
        "for n in range(len(analizar95porciento)):\n",
        "  i = analizar95porciento[n]\n",
        "  if(i in rankPos):\n",
        "    promediar1[0] += rankPos[i]\n",
        "  if(i in rankNeg):\n",
        "    promediar1[1] += rankNeg[i]\n",
        "\n",
        "data1 = promediar1 / len(analizar95porciento)\n",
        "\n",
        "plt.figure(figsize=(4,8))\n",
        "plt.bar(height=promediar1 , x=[\"Positivo\",\"Negativo\"], color = [\"blue\",\"red\"])\n",
        "plt.show()\n",
        "print(promediar1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL6GoZVaIrRK"
      },
      "source": [
        "yo de aca concluyo que tienen muchos buenos rankeados pero porque escriben muchos-> esto me lleva a descartar una hipotesis de mentiras/hacerlo para obtener puntaje alto\n",
        "\n",
        "\n",
        "\n",
        "vemos que las sumas de los suenios negativos y los positivos nos dice que el 5% que tienen muchos bien rankeados es porque escriben mucho. mientras que el 95% esta mas parejo en cuanto al rank?? nose si meter esto /// creo q no"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q85oPt_xZa8S"
      },
      "source": [
        "['cohesion', 'intent', 'lucidity', 'raicesl', 'rating', 'technique', 'url', 'user', 'dream', 'additional_comments', 'themes', 'settings', 'characters', 'emotions', 'activities']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Quiero hacer porcentajes por cada usuario q publico alguna vez algun suenio para ver como suelen ser rankeados sus suenios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSCh1sGwZbHv"
      },
      "outputs": [],
      "source": [
        "\n",
        "dicPorcentajes = dict()\n",
        "rankIT = list(rank.keys())\n",
        "for i in range(len(rank.keys())):\n",
        "  key = rankIT[i]\n",
        "  if(key in rankPos):\n",
        "    pos = (rankPos[key]*100)/rank[key]\n",
        "    pos = round(pos,1)\n",
        "  else:\n",
        "    pos = 0\n",
        "  if(key in rankNeg):\n",
        "    neg = (rankNeg[key]*100)/rank[key]\n",
        "    neg = round(neg,1)\n",
        "  else:\n",
        "    neg = 0\n",
        "  neu = 100 -(pos+neg)\n",
        "  dicPorcentajes[key] = [pos,neg,neu]\n",
        "\n",
        "\n",
        "#busqueda\n",
        "busquedaPositivos = []\n",
        "it = list(dicPorcentajes.keys())\n",
        "for i in range(len(dicPorcentajes.keys())):\n",
        "  key = it[i]\n",
        "  #if(dicPorcentajes[key][0]>dicPorcentajes[key][1] and rank[key]>8):\n",
        "  if(dicPorcentajes[key][0]>dicPorcentajes[key][1] and rank[key]>8):\n",
        "    busquedaPositivos.append([key,dicPorcentajes[key][0],dicPorcentajes[key][1],rank[key]])\n",
        "\n",
        "#busquedaPositivos#[user,%pos,%neg,cantPublicaciones]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA8ZIX6LhQW7"
      },
      "source": [
        "de hecho existen usuarios con todos sus suenios rankeados positivos pero estos no superan los 10 suenios publicados"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "O32QYyzuxsa3",
        "CkGdGHdsyQwu",
        "KtcKmEbMRNMG",
        "i4C05KbvEOOP",
        "tHSE17nt1tv8",
        "MiFXIVocRQ_3"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
