{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhd-sJ-CqlpE"
      },
      "source": [
        "#0. Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxuPRrvZrMrM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3f725c8-38d5-4a4c-9996-3a5fae551451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Importamos las librerías habituales\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import matplotlib\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O32QYyzuxsa3"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##Cómo levantar el df limpio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZYd3WMrxzWQ"
      },
      "outputs": [],
      "source": [
        "directory = '/content/drive/My Drive/DatosTP/limp'\n",
        "df = pd.read_pickle('/content/drive/My Drive/DatosTP/limp1.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zuu3ktBPx3g7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fc449c7-b0c9-45f8-aa8d-b8e9604e980b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['cohesion', 'intent', 'lucidity', 'raicesl', 'rating', 'technique',\n",
              "       'url', 'user', 'dream', 'additional_comments', 'themes', 'settings',\n",
              "       'characters', 'emotions', 'activities'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "for i in range(2,14):\n",
        "  path = directory + str(i) + '.pickle'\n",
        "  df1 = pd.read_pickle(path)\n",
        "  df = df.append(df1)\n",
        "dfRank = df\n",
        "df.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHOkoltntvxZ"
      },
      "outputs": [],
      "source": [
        "df[\"raices\"] = df[\"raicesl\"]\n",
        "df[\"text\"] = df[\"dream\"]\n",
        "\n",
        "#Junta y arma un solo \"texto\" con las raíces\n",
        "df['raices_unidas'] = df['raices'].apply(\" \".join)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YH8ySeQx8x6"
      },
      "source": [
        "Es casi igual que antes, solo que con 13 archivos limp#.pickle  \n",
        "También cambiaron las columnas:  \n",
        "['cohesion', 'intent', 'lucidity', 'raicesl', 'rating', 'technique',\n",
        "       'url', 'user', 'dream', 'additional_comments', 'themes', 'settings',\n",
        "       'characters', 'emotions', 'activities']\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuxmk0zpt5Lu"
      },
      "source": [
        "#1. Limpieza previa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfCbjX9z8G2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87445e06-4b54-450a-9076-68022ee3d383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'welcome', 'to', 'the', 'bieber', 'dream', 'i', 'm', 'no', 'hater', 'or', 'lover', 'so', 'i', 'be', 'in', 'this', 'strange', 'house', 'i', 'don', 't', 'know', 'of', 'and', 'a', 'a', 'concert', 'go', 'on', 'and', 'all', 'these', 'hobo', 'star', 'at', 'me', 'like', 'i', 'm', 'a', 'ball', 'of', 'cheese', 'wait', 'any', 'self', 'concious', 'in', 'this', 'one', 'then', 'he', 'walk', 'out', 'gasp', 'lover', 'faint', 'and', 'hater', 'shoot', 'him', 'in', 'the', 'head', 'jk', 'actually', 'he', 'be', 'quite', 'old', 'and', 'less', 'wimpy', 'look', 'and', 'amazingly', 'he', 'doesn', 't', 'have', 'that', 'kid', 'voice', 'and', 'hair', 'flip', 'thing', 'go', 'on', 'yeah', 'i', 'hate', 'to', 'admit', 'it', 'but', 'he', 'be', 'cute', 'so', 'i', 'felt', 'a', 'if', 'it', 'be', 'real', 'so', 'i', 'take', 'it', 'slow', 'i', 'be', 'just', 'talk', 'or', 'flirt', 'with', 'him', 'and', 'he', 'let', 'me', 'in', 'his', 'limo', 'and', 'jeesh', 'this', 'quite', 'embarassing', 'because', 'i', 'm', 'not', 'even', 'fond', 'of', 'him', 'anyway', 'i', 'be', 'lead', 'in', 'his', 'house', 'and', 'it', 'be', 'actually', 'quite', 'normal', 'no', 'scream', 'little', 'girls', 'no', 'paris', 'or', 'whatever', 'nothing', 'really', 'famous', 'or', 'huge', 'or', 'expensive', 'so', 'i', 'be', 'in', 'his', 'room', 'and', 'i', 'can', 't', 'exactly', 'remember', 'everything', 'but', 'it', 'be', 'quite', 'normal', 'too', 'at', 'that', 'time', 'i', 'actually', 'want', 'to', 'kiss', 'him', 'i', 'get', 'ready', 'about', 'to', 'do', 'it', 'then', 'my', 'dog', 'whine', 'for', 'her', 'to', 'go', 'potty', 'crap']\n",
            "['buzz', 'and', 'jessie', 'from', 'the', 'toy', 'story', 'movie', 'be', 'this', 'close', 'to', 'kiss', 'then', 'i', 'wake', 'up']\n",
            "http://www.dreamjournal.net/journal/dream/dream_id/155289/username/Dragon12\n"
          ]
        }
      ],
      "source": [
        "#Hay algunos repetidos...\n",
        "print(df[\"raices\"][35427])\n",
        "print(df[\"raices\"][35428])\n",
        "print(df[\"url\"][35427])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnwNfoS0t_H3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc37f97a-7671-4ceb-f1ac-448c0c6cd2cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        cohesion intent  lucidity  \\\n",
              "0            2.0     -1     -1.00   \n",
              "1            4.0     -1     -1.00   \n",
              "2            3.0     -1     -1.00   \n",
              "3            5.0     -1     -1.00   \n",
              "4            4.0    red      1.25   \n",
              "...          ...    ...       ...   \n",
              "123718       5.0     -1     -1.00   \n",
              "123719       4.0     -1     -1.00   \n",
              "123720       3.0     -1     -1.00   \n",
              "123721       3.0     -1     -1.00   \n",
              "123722       4.0    red      2.50   \n",
              "\n",
              "                                                  raicesl  rating technique  \\\n",
              "0       [i, be, on, the, ground, floor, of, a, dorm, b...     3.0        -1   \n",
              "1       [i, be, leave, a, job, after, a, shift, have, ...     2.0        -1   \n",
              "2       [twilight, my, present, home, i, be, leave, th...     3.0        -1   \n",
              "3       [when, your, favourite, song, be, announce, in...     2.0        -1   \n",
              "4       [serve, customer, be, in, a, mall, style, like...     2.0        -1   \n",
              "...                                                   ...     ...       ...   \n",
              "123718  [first, off, ali, do, not, have, the, father, ...    -1.0        -1   \n",
              "123719  [ali, ben, dan, and, i, be, all, in, the, clas...    -1.0        -1   \n",
              "123720  [i, be, in, some, sort, of, a, strange, arena,...    -1.0        -1   \n",
              "123721  [thois, be, suuuch, a, cool, dream, too, bad, ...    -1.0        -1   \n",
              "123722  [i, live, on, a, beach, in, a, very, lovely, h...    -1.0        -1   \n",
              "\n",
              "                                                      url             user  \\\n",
              "0       http://www.dreamjournal.net/journal/dream/drea...             haux   \n",
              "1       http://www.dreamjournal.net/journal/dream/drea...             haux   \n",
              "2       http://www.dreamjournal.net/journal/dream/drea...       PearlDiver   \n",
              "3       http://www.dreamjournal.net/journal/dream/drea...  LucidDreamer777   \n",
              "4       http://www.dreamjournal.net/journal/dream/drea...  LucidDreamer777   \n",
              "...                                                   ...              ...   \n",
              "123718  http://www.dreamjournal.net/journal/dream/drea...        MtDewWolf   \n",
              "123719  http://www.dreamjournal.net/journal/dream/drea...        MtDewWolf   \n",
              "123720  http://www.dreamjournal.net/journal/dream/drea...  Elemental_angel   \n",
              "123721  http://www.dreamjournal.net/journal/dream/drea...     MyBounciness   \n",
              "123722  http://www.dreamjournal.net/journal/dream/drea...          toric13   \n",
              "\n",
              "                                                    dream  \\\n",
              "0        i was on the ground floor of a dorm building ...   \n",
              "1        i was leaving a job after a shift had ended i...   \n",
              "2        twilight my present home i am leaving the hou...   \n",
              "3        when your favourite song is announced in your...   \n",
              "4        serving customers being in a mall styled like...   \n",
              "...                                                   ...   \n",
              "123718   first off ali did not have the father she doe...   \n",
              "123719   ali ben dan and i were all in the classroom a...   \n",
              "123720   i was in some sort of a strange arena facing ...   \n",
              "123721   thois is suuuch a cool dream too bad i dun re...   \n",
              "123722   i lived on a beach in a very lovely house exp...   \n",
              "\n",
              "                                      additional_comments  \\\n",
              "0                                                           \n",
              "1                                                           \n",
              "2       curious dream bl is one of the worst people i ...   \n",
              "3       p s the last song that i played before going t...   \n",
              "4                                                morning    \n",
              "...                                                   ...   \n",
              "123718   omg if this is precognitive i m killing someone    \n",
              "123719                                                      \n",
              "123720                  do not know where this came from    \n",
              "123721  well elavators symbloize rising up or down on ...   \n",
              "123722                                                      \n",
              "\n",
              "                                               themes  \\\n",
              "0                                           friendly    \n",
              "1                                   failure friendly    \n",
              "2       music action success failure health friendly    \n",
              "3                                          music fun    \n",
              "4                                          nightmare    \n",
              "...                                               ...   \n",
              "123718                                                  \n",
              "123719                                                  \n",
              "123720                                      violence    \n",
              "123721                            nightmare violence    \n",
              "123722                                                  \n",
              "\n",
              "                                                 settings  \\\n",
              "0                 town city indoors distorted unfamiliar    \n",
              "1                   indoors distorted familiar ambiguous    \n",
              "2       outdoors indoors distorted familiar unfamiliar...   \n",
              "3                                        school outdoors    \n",
              "4                                  mall outdoors indoors    \n",
              "...                                                   ...   \n",
              "123718                                            school    \n",
              "123719                                            school    \n",
              "123720                                                      \n",
              "123721                                                      \n",
              "123722                                              home    \n",
              "\n",
              "                                          characters  \\\n",
              "0                                  friend colleague    \n",
              "1                              colleague unfamiliar    \n",
              "2       other relative s friend stranger unfamiliar    \n",
              "3                                 colleague teacher    \n",
              "4                   child other relative s stranger    \n",
              "...                                              ...   \n",
              "123718                      other relative s friend    \n",
              "123719                                       friend    \n",
              "123720                               friend animals    \n",
              "123721                             other relative s    \n",
              "123722                                       friend    \n",
              "\n",
              "                                     emotions  \\\n",
              "0                                emotionless    \n",
              "1                                emotionless    \n",
              "2                     worry relaxed peaceful    \n",
              "3                 fear dread happiness shock    \n",
              "4       sadness worry fear dread emotionless    \n",
              "...                                       ...   \n",
              "123718            sadness fear dread anxiety    \n",
              "123719                             confusion    \n",
              "123720                             confusion    \n",
              "123721                              peaceful    \n",
              "123722                        worry peaceful    \n",
              "\n",
              "                                               activities  \\\n",
              "0               physical thinking visual location change    \n",
              "1               thinking visual movement location change    \n",
              "2       auditory physical thinking visual movement pro...   \n",
              "3       auditory thinking visual movement expressive c...   \n",
              "4       auditory physical thinking visual movement sea...   \n",
              "...                                                   ...   \n",
              "123718                                                      \n",
              "123719                                         searching    \n",
              "123720                                         searching    \n",
              "123721                                                      \n",
              "123722                                                      \n",
              "\n",
              "                                                   raices  \\\n",
              "0       [i, be, on, the, ground, floor, of, a, dorm, b...   \n",
              "1       [i, be, leave, a, job, after, a, shift, have, ...   \n",
              "2       [twilight, my, present, home, i, be, leave, th...   \n",
              "3       [when, your, favourite, song, be, announce, in...   \n",
              "4       [serve, customer, be, in, a, mall, style, like...   \n",
              "...                                                   ...   \n",
              "123718  [first, off, ali, do, not, have, the, father, ...   \n",
              "123719  [ali, ben, dan, and, i, be, all, in, the, clas...   \n",
              "123720  [i, be, in, some, sort, of, a, strange, arena,...   \n",
              "123721  [thois, be, suuuch, a, cool, dream, too, bad, ...   \n",
              "123722  [i, live, on, a, beach, in, a, very, lovely, h...   \n",
              "\n",
              "                                                     text  \\\n",
              "0        i was on the ground floor of a dorm building ...   \n",
              "1        i was leaving a job after a shift had ended i...   \n",
              "2        twilight my present home i am leaving the hou...   \n",
              "3        when your favourite song is announced in your...   \n",
              "4        serving customers being in a mall styled like...   \n",
              "...                                                   ...   \n",
              "123718   first off ali did not have the father she doe...   \n",
              "123719   ali ben dan and i were all in the classroom a...   \n",
              "123720   i was in some sort of a strange arena facing ...   \n",
              "123721   thois is suuuch a cool dream too bad i dun re...   \n",
              "123722   i lived on a beach in a very lovely house exp...   \n",
              "\n",
              "                                            raices_unidas  \n",
              "0       i be on the ground floor of a dorm building th...  \n",
              "1       i be leave a job after a shift have end it be ...  \n",
              "2       twilight my present home i be leave the house ...  \n",
              "3       when your favourite song be announce in your d...  \n",
              "4       serve customer be in a mall style like an anci...  \n",
              "...                                                   ...  \n",
              "123718  first off ali do not have the father she do no...  \n",
              "123719  ali ben dan and i be all in the classroom acro...  \n",
              "123720  i be in some sort of a strange arena face a bl...  \n",
              "123721  thois be suuuch a cool dream too bad i dun rem...  \n",
              "123722  i live on a beach in a very lovely house expen...  \n",
              "\n",
              "[123723 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-93f77710-8e04-4ef8-b5d1-1b02c79e7e8d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cohesion</th>\n",
              "      <th>intent</th>\n",
              "      <th>lucidity</th>\n",
              "      <th>raicesl</th>\n",
              "      <th>rating</th>\n",
              "      <th>technique</th>\n",
              "      <th>url</th>\n",
              "      <th>user</th>\n",
              "      <th>dream</th>\n",
              "      <th>additional_comments</th>\n",
              "      <th>themes</th>\n",
              "      <th>settings</th>\n",
              "      <th>characters</th>\n",
              "      <th>emotions</th>\n",
              "      <th>activities</th>\n",
              "      <th>raices</th>\n",
              "      <th>text</th>\n",
              "      <th>raices_unidas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>[i, be, on, the, ground, floor, of, a, dorm, b...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>http://www.dreamjournal.net/journal/dream/drea...</td>\n",
              "      <td>haux</td>\n",
              "      <td>i was on the ground floor of a dorm building ...</td>\n",
              "      <td></td>\n",
              "      <td>friendly</td>\n",
              "      <td>town city indoors distorted unfamiliar</td>\n",
              "      <td>friend colleague</td>\n",
              "      <td>emotionless</td>\n",
              "      <td>physical thinking visual location change</td>\n",
              "      <td>[i, be, on, the, ground, floor, of, a, dorm, b...</td>\n",
              "      <td>i was on the ground floor of a dorm building ...</td>\n",
              "      <td>i be on the ground floor of a dorm building th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>[i, be, leave, a, job, after, a, shift, have, ...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>http://www.dreamjournal.net/journal/dream/drea...</td>\n",
              "      <td>haux</td>\n",
              "      <td>i was leaving a job after a shift had ended i...</td>\n",
              "      <td></td>\n",
              "      <td>failure friendly</td>\n",
              "      <td>indoors distorted familiar ambiguous</td>\n",
              "      <td>colleague unfamiliar</td>\n",
              "      <td>emotionless</td>\n",
              "      <td>thinking visual movement location change</td>\n",
              "      <td>[i, be, leave, a, job, after, a, shift, have, ...</td>\n",
              "      <td>i was leaving a job after a shift had ended i...</td>\n",
              "      <td>i be leave a job after a shift have end it be ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>[twilight, my, present, home, i, be, leave, th...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>http://www.dreamjournal.net/journal/dream/drea...</td>\n",
              "      <td>PearlDiver</td>\n",
              "      <td>twilight my present home i am leaving the hou...</td>\n",
              "      <td>curious dream bl is one of the worst people i ...</td>\n",
              "      <td>music action success failure health friendly</td>\n",
              "      <td>outdoors indoors distorted familiar unfamiliar...</td>\n",
              "      <td>other relative s friend stranger unfamiliar</td>\n",
              "      <td>worry relaxed peaceful</td>\n",
              "      <td>auditory physical thinking visual movement pro...</td>\n",
              "      <td>[twilight, my, present, home, i, be, leave, th...</td>\n",
              "      <td>twilight my present home i am leaving the hou...</td>\n",
              "      <td>twilight my present home i be leave the house ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>[when, your, favourite, song, be, announce, in...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>http://www.dreamjournal.net/journal/dream/drea...</td>\n",
              "      <td>LucidDreamer777</td>\n",
              "      <td>when your favourite song is announced in your...</td>\n",
              "      <td>p s the last song that i played before going t...</td>\n",
              "      <td>music fun</td>\n",
              "      <td>school outdoors</td>\n",
              "      <td>colleague teacher</td>\n",
              "      <td>fear dread happiness shock</td>\n",
              "      <td>auditory thinking visual movement expressive c...</td>\n",
              "      <td>[when, your, favourite, song, be, announce, in...</td>\n",
              "      <td>when your favourite song is announced in your...</td>\n",
              "      <td>when your favourite song be announce in your d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.0</td>\n",
              "      <td>red</td>\n",
              "      <td>1.25</td>\n",
              "      <td>[serve, customer, be, in, a, mall, style, like...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>http://www.dreamjournal.net/journal/dream/drea...</td>\n",
              "      <td>LucidDreamer777</td>\n",
              "      <td>serving customers being in a mall styled like...</td>\n",
              "      <td>morning</td>\n",
              "      <td>nightmare</td>\n",
              "      <td>mall outdoors indoors</td>\n",
              "      <td>child other relative s stranger</td>\n",
              "      <td>sadness worry fear dread emotionless</td>\n",
              "      <td>auditory physical thinking visual movement sea...</td>\n",
              "      <td>[serve, customer, be, in, a, mall, style, like...</td>\n",
              "      <td>serving customers being in a mall styled like...</td>\n",
              "      <td>serve customer be in a mall style like an anci...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123718</th>\n",
              "      <td>5.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>[first, off, ali, do, not, have, the, father, ...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>http://www.dreamjournal.net/journal/dream/drea...</td>\n",
              "      <td>MtDewWolf</td>\n",
              "      <td>first off ali did not have the father she doe...</td>\n",
              "      <td>omg if this is precognitive i m killing someone</td>\n",
              "      <td></td>\n",
              "      <td>school</td>\n",
              "      <td>other relative s friend</td>\n",
              "      <td>sadness fear dread anxiety</td>\n",
              "      <td></td>\n",
              "      <td>[first, off, ali, do, not, have, the, father, ...</td>\n",
              "      <td>first off ali did not have the father she doe...</td>\n",
              "      <td>first off ali do not have the father she do no...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123719</th>\n",
              "      <td>4.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>[ali, ben, dan, and, i, be, all, in, the, clas...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>http://www.dreamjournal.net/journal/dream/drea...</td>\n",
              "      <td>MtDewWolf</td>\n",
              "      <td>ali ben dan and i were all in the classroom a...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>school</td>\n",
              "      <td>friend</td>\n",
              "      <td>confusion</td>\n",
              "      <td>searching</td>\n",
              "      <td>[ali, ben, dan, and, i, be, all, in, the, clas...</td>\n",
              "      <td>ali ben dan and i were all in the classroom a...</td>\n",
              "      <td>ali ben dan and i be all in the classroom acro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123720</th>\n",
              "      <td>3.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>[i, be, in, some, sort, of, a, strange, arena,...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>http://www.dreamjournal.net/journal/dream/drea...</td>\n",
              "      <td>Elemental_angel</td>\n",
              "      <td>i was in some sort of a strange arena facing ...</td>\n",
              "      <td>do not know where this came from</td>\n",
              "      <td>violence</td>\n",
              "      <td></td>\n",
              "      <td>friend animals</td>\n",
              "      <td>confusion</td>\n",
              "      <td>searching</td>\n",
              "      <td>[i, be, in, some, sort, of, a, strange, arena,...</td>\n",
              "      <td>i was in some sort of a strange arena facing ...</td>\n",
              "      <td>i be in some sort of a strange arena face a bl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123721</th>\n",
              "      <td>3.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>[thois, be, suuuch, a, cool, dream, too, bad, ...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>http://www.dreamjournal.net/journal/dream/drea...</td>\n",
              "      <td>MyBounciness</td>\n",
              "      <td>thois is suuuch a cool dream too bad i dun re...</td>\n",
              "      <td>well elavators symbloize rising up or down on ...</td>\n",
              "      <td>nightmare violence</td>\n",
              "      <td></td>\n",
              "      <td>other relative s</td>\n",
              "      <td>peaceful</td>\n",
              "      <td></td>\n",
              "      <td>[thois, be, suuuch, a, cool, dream, too, bad, ...</td>\n",
              "      <td>thois is suuuch a cool dream too bad i dun re...</td>\n",
              "      <td>thois be suuuch a cool dream too bad i dun rem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123722</th>\n",
              "      <td>4.0</td>\n",
              "      <td>red</td>\n",
              "      <td>2.50</td>\n",
              "      <td>[i, live, on, a, beach, in, a, very, lovely, h...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>http://www.dreamjournal.net/journal/dream/drea...</td>\n",
              "      <td>toric13</td>\n",
              "      <td>i lived on a beach in a very lovely house exp...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>home</td>\n",
              "      <td>friend</td>\n",
              "      <td>worry peaceful</td>\n",
              "      <td></td>\n",
              "      <td>[i, live, on, a, beach, in, a, very, lovely, h...</td>\n",
              "      <td>i lived on a beach in a very lovely house exp...</td>\n",
              "      <td>i live on a beach in a very lovely house expen...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>123723 rows × 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-93f77710-8e04-4ef8-b5d1-1b02c79e7e8d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-93f77710-8e04-4ef8-b5d1-1b02c79e7e8d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-93f77710-8e04-4ef8-b5d1-1b02c79e7e8d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Regresor\n",
        "\n"
      ],
      "metadata": {
        "id": "LMilvutMLNOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfRank = df.loc[df['rating']>0]\n",
        "dfPosRank = dfRank.loc[dfRank[\"rating\"]>3] #asumo rank pos == mayor a 3\n",
        "dfNegRank = dfRank.loc[dfRank[\"rating\"]<3] #asumo rank neg == menor a 3\n",
        "\n",
        "dfPosRank['rating']=1\n",
        "dfNegRank['rating']=0\n",
        "\n",
        "\n",
        "dataPos = dfPosRank[['rating','text']]\n",
        "dataNeg = dfNegRank[['rating','text']]\n",
        "\n",
        "data = dataPos.append(dataNeg)\n",
        "data.sort_index\n",
        "data = data[['text','rating']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x7JScy-LSIc",
        "outputId": "6b25ef72-ba38-4790-bbec-0277a031aaa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download()\n",
        "from nltk import word_tokenize\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = (stopwords.words('english'))\n",
        "stopwords += ['get','go','one','like','look','say', 'see', 'back','seen','talk','came','come','may','tell','dream','tumblr', '__']\n",
        "\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5XkrrF7YMY6L",
        "outputId": "601f3abe-a207-4345-e8a1-dba1949c64d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> \n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "Hit Enter to continue: \n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "Hit Enter to continue: \n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "Hit Enter to continue: \n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "Hit Enter to continue: \n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet2021......... Open English Wordnet 2021\n",
            "  [ ] wordnet31........... Wordnet 3.1\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "Hit Enter to continue: \n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "Hit Enter to continue: \n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "Hit Enter to continue: \n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "Hit Enter to continue: \n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "Hit Enter to continue: \n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet2021......... Open English Wordnet 2021\n",
            "  [ ] wordnet31........... Wordnet 3.1\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "Hit Enter to continue: \n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> \n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-90c59373f478>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'go'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'one'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'like'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'look'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'say'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'see'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'back'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'seen'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'talk'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'came'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'come'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'may'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tell'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dream'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tumblr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(data.text, data.rating, random_state=22)"
      ],
      "metadata": {
        "id": "FzSmxLg5rGHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zAI-e_b0rZCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_cleaning(a):\n",
        "  remove_punctuation = [char for char in a if char not in string.punctuation]\n",
        "  remove_punctuation=''.join(remove_punctuation)\n",
        "  return[word for word in remove_punctuation.split() if word.lower() not in stopwords]"
      ],
      "metadata": {
        "id": "Z5JFROx6iVz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.iloc[:,0].apply(text_cleaning))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-_2vCW7lWa5",
        "outputId": "6e526a93-4bba-4c7e-c2de-513a09c9ead6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26506    [high, school, sitting, classes, working, hard...\n",
            "39773    [really, fuzzy, really, hard, remember, guess,...\n",
            "13083    [indoors, flat, flat, totally, unfamiliar, rem...\n",
            "11823    [fell, asleep, tv, woke, infomercial, wen, cav...\n",
            "52321    [walking, hallway, thought, old, school, mine,...\n",
            "                               ...                        \n",
            "45289    [man, trying, house, across, street, knife, ho...\n",
            "41365    [large, indoor, place, sitting, talking, someo...\n",
            "50979    [hour, nap, old, friend, familys, house, kind,...\n",
            "44069    [sleeping, bedroom, parent, house, nude, thoug...\n",
            "5952     [morning, january, saturday, fishing, wife, yo...\n",
            "Name: text, Length: 19437, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#bow_transformer = CountVectorizer(analyzer = text_cleaning).fit(data['text'])\n",
        "\n",
        "###bow_transformer = CountVectorizer(analyzer = text_cleaning).fit(x_train)\n",
        "\n",
        "vect = CountVectorizer(analyzer = text_cleaning)\n",
        "\n",
        "X_train = vect.fit_transform(x_train)\n",
        "\n",
        "X_test = vect.transform(x_test)\n",
        "\n",
        "#tengo mis dudas\n",
        "###bow_transformer1 = CountVectorizer(analyzer = text_cleaning).fit(x_train)\n"
      ],
      "metadata": {
        "id": "HPPvWu2Plf6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#title_bow = bow_transformer.transform(data['text'])\n",
        "\n",
        "####title_bow = bow_transformer.transform(x_train)\n",
        "\n",
        "\n",
        "#tengo mis dudas\n",
        "####X_test = bow_transformer1.transform(x_test)"
      ],
      "metadata": {
        "id": "Rpe69AAonVHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "#tfidf_transformer = TfidfTransformer().fit(title_bow)\n",
        "\n",
        "#title_tfidf = tfidf_transformer.transform(title_bow)\n",
        "\n",
        "\n",
        "#pepe_transformer = TfidfTransformer().fit(X_test)\n",
        "#title_pepe = pepe_transformer.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "XX_train = TfidfTransformer().fit_transform(X_train)\n",
        "XX_test = TfidfTransformer().fit_transform(X_test)\n"
      ],
      "metadata": {
        "id": "1L2CEy4Fm7ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "#model = MultinomialNB().fit(title_tfidf,data['rating'])\n",
        "\n",
        "mnb = MultinomialNB()\n",
        "\n",
        "mnb.fit(XX_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Fe2imV0oWm0",
        "outputId": "f311ab67-f2a9-4ba5-89e8-a4a466d71c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#all_predictions = model.predict(title_tfidf)\n",
        "\n",
        "result = mnb.predict(XX_test)"
      ],
      "metadata": {
        "id": "eTeB1AsqopNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy\n",
        "print(accuracy_score(result,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oS0wvDfPs9fr",
        "outputId": "a77f48bf-b954-44e4-ae6e-6071ed060f27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6590523228893348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "y_hat_test = mnb.predict_proba(XX_test)[:, 1]\n",
        "y_hat_train = mnb.predict_proba(XX_train)[:, 1]\n",
        "\n",
        "# evaluo el AUC\n",
        "roc_test = roc_auc_score(y_test, y_hat_test)\n",
        "roc_train = roc_auc_score(y_train, y_hat_train)\n",
        "roc_test, roc_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCcCFasx2aBj",
        "outputId": "e80e7276-2c1f-4d9f-f315-7a81449c10ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6611470982376809, 0.814750195712374)"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#confusion_matrix(data['rating'],all_predictions)\n",
        "\n",
        "cm =confusion_matrix(y_test, result)\n",
        "probas = mnb.predict_proba(XX_test)"
      ],
      "metadata": {
        "id": "WKDUB_IVozV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TPR_FPR(cm): # computa la tasa de verdaderos y falsos positivos a partir de la matriz de confusion\n",
        "  TPR = cm[1,1]/(cm[1,1]+cm[0,1])\n",
        "  FPR = cm[1,0]/(cm[1,0]+cm[0,0])\n",
        "  return [TPR,FPR] # devuelve una lista donde el primer elemento es la tasa de verdaderos positivos y el segundo la tasa de falsos negativos\n",
        "\n",
        "T = 0.5 # definimos el umbral\n",
        "ypred = np.zeros(probas.shape[0]) # armamos un vector que tiene todos ceros y tiene la longitud del vector de etiquetas\n",
        "ypred[probas[:,1]>T] = 1 # donde la probabilidad es mayor que T, ponemos 1\n",
        "cm = confusion_matrix(ypred, y_test) # metemos en la matriz de confusion\n",
        "metricas = TPR_FPR(cm)\n",
        "\n",
        "print('Matriz de confusion del modelo es:')\n",
        "print(cm)\n",
        "print('Tasa de verdaderos positivos: {}'.format(round(metricas[0],4)))\n",
        "print('Tasa de falsos positivos: {}'.format(round(metricas[1],4)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ6Zh0EC35Oh",
        "outputId": "b400f009-ba61-418c-f0a8-a0c5aac76f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de confusion del modelo es:\n",
            "[[4166 2199]\n",
            " [  10  104]]\n",
            "Tasa de verdaderos positivos: 0.0452\n",
            "Tasa de falsos positivos: 0.0024\n"
          ]
        }
      ]
    }
  ]
}